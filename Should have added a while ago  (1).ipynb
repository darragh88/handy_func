{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9f801fc",
   "metadata": {},
   "source": [
    "### Transforms on the Target Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be6167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "class SignedLogTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Signed‑log (log‑modulus) transform:\n",
    "      y' = sign(y) * log1p(abs(y))\n",
    "    Inverse:\n",
    "      y  = sign(y') * (exp(|y'|) - 1)\n",
    "    \"\"\"\n",
    "    def fit(self, X=None, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, y):\n",
    "        y = np.asarray(y, dtype=float)\n",
    "        return np.sign(y) * np.log1p(np.abs(y))\n",
    "\n",
    "    def inverse_transform(self, y_prime):\n",
    "        y_prime = np.asarray(y_prime, dtype=float)\n",
    "        return np.sign(y_prime) * (np.expm1(np.abs(y_prime)))\n",
    "\n",
    "\n",
    "class AsinhTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Hyperbolic arcsine transform:\n",
    "      y' = arcsinh(y / c)\n",
    "    Inverse:\n",
    "      y = sinh(y') * c\n",
    "    \"\"\"\n",
    "    def __init__(self, c=None):\n",
    "        # c = scale parameter; if None, will be set to median(|y|)\n",
    "        self.c = c\n",
    "\n",
    "    def fit(self, X=None, y=None):\n",
    "        y = np.asarray(y, dtype=float).ravel()\n",
    "        if self.c is None:\n",
    "            # avoid c == 0\n",
    "            self.c = np.median(np.abs(y)) or 1.0\n",
    "        return self\n",
    "\n",
    "    def transform(self, y):\n",
    "        y = np.asarray(y, dtype=float)\n",
    "        return np.arcsinh(y / self.c)\n",
    "\n",
    "    def inverse_transform(self, y_prime):\n",
    "        y_prime = np.asarray(y_prime, dtype=float)\n",
    "        return np.sinh(y_prime) * self.c\n",
    "\n",
    "\n",
    "class YeoJohnsonTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Yeo‑Johnson power transform (handles negatives):\n",
    "      Fits lambda by MLE under Gaussian assumption.\n",
    "    \"\"\"\n",
    "    def __init__(self, standardize=False):\n",
    "        self.standardize = standardize\n",
    "        self.pt = PowerTransformer(method='yeo-johnson', standardize=self.standardize)\n",
    "\n",
    "    def fit(self, X=None, y=None):\n",
    "        y = np.asarray(y, dtype=float).reshape(-1, 1)\n",
    "        self.pt.fit(y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, y):\n",
    "        y = np.asarray(y, dtype=float).reshape(-1, 1)\n",
    "        return self.pt.transform(y).flatten()\n",
    "\n",
    "    def inverse_transform(self, y_prime):\n",
    "        y_prime = np.asarray(y_prime, dtype=float).reshape(-1, 1)\n",
    "        return self.pt.inverse_transform(y_prime).flatten()\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate some synthetic heavy‑tailed data\n",
    "    y = np.random.standard_t(df=2, size=1000) * 50\n",
    "\n",
    "    # 1) Signed‑log\n",
    "    sl = SignedLogTransformer()\n",
    "    y_sl = sl.fit_transform(y)\n",
    "    y_sl_inv = sl.inverse_transform(y_sl)\n",
    "\n",
    "    # 2) Asinh\n",
    "    a = AsinhTransformer()\n",
    "    y_a = a.fit_transform(y)\n",
    "    y_a_inv = a.inverse_transform(y_a)\n",
    "\n",
    "    # 3) Yeo‑Johnson\n",
    "    yj = YeoJohnsonTransformer()\n",
    "    y_yj = yj.fit_transform(y)\n",
    "    y_yj_inv = yj.inverse_transform(y_yj)\n",
    "\n",
    "    # Check that round‑trip errors are near zero\n",
    "    print(\"Signed‑log error:\", np.max(np.abs(y - y_sl_inv)))\n",
    "    print(\"Asinh error:    \", np.max(np.abs(y - y_a_inv)))\n",
    "    print(\"Yeo‑Johnson error:\", np.max(np.abs(y - y_yj_inv)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18cb9af",
   "metadata": {},
   "source": [
    "### Error diagnostic plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df60acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Union, Callable, Optional\n",
    "\n",
    "def error_heatmap(\n",
    "    err: np.ndarray,\n",
    "    x1: np.ndarray,\n",
    "    x2: np.ndarray,\n",
    "    bins: Union[int, Tuple[int, int]] = (10, 10),\n",
    "    quantile_bins: bool = True,\n",
    "    min_count: int = 25,\n",
    "    agg: Union[str, Callable[[np.ndarray], float]] = \"mae\",\n",
    "    cmap: str = \"YlOrRd\",\n",
    "    annotate: bool = True,\n",
    "    title: Optional[str] = None,\n",
    "    x1_name: str = \"x1\",\n",
    "    x2_name: str = \"x2\",\n",
    "    figsize: Tuple[int, int] = (10, 7),\n",
    "):\n",
    "    \"\"\"\n",
    "    Make a 2D heatmap of an error metric across two continuous features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    err : array-like\n",
    "        Error per observation. Pass residuals (y - y_hat) or absolute error.\n",
    "    x1, x2 : array-like\n",
    "        Feature values aligned with `err`.\n",
    "    bins : int or (int, int)\n",
    "        Number of bins for (x1, x2).\n",
    "    quantile_bins : bool\n",
    "        If True, use quantile (equal-count) bins; else equal-width bins.\n",
    "    min_count : int\n",
    "        Cells with < min_count observations are masked.\n",
    "    agg : {\"mae\",\"mse\",\"rmse\",\"mean\",\"median\"} or callable\n",
    "        Aggregation for the cell. If callable, it receives a 1D ndarray of `err`.\n",
    "        Common choices:\n",
    "          - \"mae\": mean absolute error (default)\n",
    "          - \"mse\": mean squared error\n",
    "          - \"rmse\": root mean squared error\n",
    "          - \"mean\": mean of err (signed)\n",
    "          - \"median\": median absolute error\n",
    "    cmap : str\n",
    "        Matplotlib colormap.\n",
    "    annotate : bool\n",
    "        If True, write the value and count in each populated cell.\n",
    "    title : str\n",
    "        Optional plot title.\n",
    "    x1_name, x2_name : str\n",
    "        Axis labels.\n",
    "    figsize : (w, h)\n",
    "        Figure size in inches.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fig, ax, tables : (matplotlib.figure.Figure, matplotlib.axes.Axes, dict)\n",
    "        tables = {\"value\": value_df, \"count\": count_df, \"x1_bins\": x1_bins, \"x2_bins\": x2_bins}\n",
    "    \"\"\"\n",
    "\n",
    "    err = np.asarray(err).ravel()\n",
    "    x1 = np.asarray(x1).ravel()\n",
    "    x2 = np.asarray(x2).ravel()\n",
    "    assert err.shape == x1.shape == x2.shape, \"err, x1, x2 must have same length\"\n",
    "\n",
    "    if isinstance(bins, int):\n",
    "        b1 = b2 = bins\n",
    "    else:\n",
    "        b1, b2 = bins\n",
    "\n",
    "    # define aggregator\n",
    "    def _agg(vals: np.ndarray) -> float:\n",
    "        if callable(agg):\n",
    "            return float(agg(vals))\n",
    "        a = agg.lower()\n",
    "        if a == \"mae\":\n",
    "            return float(np.mean(np.abs(vals)))\n",
    "        if a == \"mse\":\n",
    "            return float(np.mean(vals ** 2))\n",
    "        if a == \"rmse\":\n",
    "            return float(np.sqrt(np.mean(vals ** 2)))\n",
    "        if a == \"mean\":\n",
    "            return float(np.mean(vals))\n",
    "        if a == \"median\":\n",
    "            return float(np.median(np.abs(vals)))\n",
    "        raise ValueError(f\"Unknown agg: {agg}\")\n",
    "\n",
    "    # bin edges\n",
    "    def _edges(v, k):\n",
    "        if quantile_bins:\n",
    "            qs = np.linspace(0, 1, k + 1)\n",
    "            # ensure unique edges\n",
    "            e = np.unique(np.quantile(v, qs))\n",
    "            # if too many duplicates (constant regions), fall back to equal-width\n",
    "            if e.size < k + 1:\n",
    "                vmin, vmax = np.min(v), np.max(v)\n",
    "                e = np.linspace(vmin, vmax, k + 1)\n",
    "        else:\n",
    "            vmin, vmax = np.min(v), np.max(v)\n",
    "            e = np.linspace(vmin, vmax, k + 1)\n",
    "        return e\n",
    "\n",
    "    e1 = _edges(x1, b1)\n",
    "    e2 = _edges(x2, b2)\n",
    "\n",
    "    # assign bins\n",
    "    binned1 = pd.cut(x1, e1, include_lowest=True, duplicates=\"drop\")\n",
    "    binned2 = pd.cut(x2, e2, include_lowest=True, duplicates=\"drop\")\n",
    "\n",
    "    df = pd.DataFrame({x1_name: binned1, x2_name: binned2, \"err\": err})\n",
    "    # aggregate\n",
    "    val_tbl = df.pivot_table(\n",
    "        values=\"err\",\n",
    "        index=x2_name,\n",
    "        columns=x1_name,\n",
    "        aggfunc=_agg,\n",
    "        dropna=False,\n",
    "    )\n",
    "    cnt_tbl = df.pivot_table(\n",
    "        values=\"err\",\n",
    "        index=x2_name,\n",
    "        columns=x1_name,\n",
    "        aggfunc=\"count\",\n",
    "        dropna=False,\n",
    "    )\n",
    "\n",
    "    # mask sparse cells\n",
    "    masked_vals = val_tbl.where(cnt_tbl >= min_count)\n",
    "\n",
    "    # plotting\n",
    "    fig, ax = plt.subplots(figsize=figsize, dpi=150)\n",
    "    im = ax.imshow(\n",
    "        masked_vals.values,\n",
    "        origin=\"lower\",\n",
    "        aspect=\"auto\",\n",
    "        cmap=cmap,\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "    cbar_label = {\n",
    "        \"mae\": \"Mean Absolute Error\",\n",
    "        \"mse\": \"Mean Squared Error\",\n",
    "        \"rmse\": \"Root MSE\",\n",
    "        \"mean\": \"Mean Error\",\n",
    "        \"median\": \"Median Absolute Error\",\n",
    "    }.get(str(agg).lower(), \"Cell Value\")\n",
    "    cbar = fig.colorbar(im, ax=ax)\n",
    "    cbar.set_label(cbar_label)\n",
    "\n",
    "    # ticks & labels\n",
    "    ax.set_xlabel(x1_name)\n",
    "    ax.set_ylabel(x2_name)\n",
    "    x_labels = [str(c) for c in masked_vals.columns]\n",
    "    y_labels = [str(r) for r in masked_vals.index]\n",
    "    ax.set_xticks(np.arange(len(x_labels)))\n",
    "    ax.set_yticks(np.arange(len(y_labels)))\n",
    "    ax.set_xticklabels(x_labels, rotation=90)\n",
    "    ax.set_yticklabels(y_labels)\n",
    "\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "\n",
    "    # annotations\n",
    "    if annotate:\n",
    "        nrows, ncols = masked_vals.shape\n",
    "        for i in range(nrows):\n",
    "            for j in range(ncols):\n",
    "                val = masked_vals.iat[i, j]\n",
    "                n = cnt_tbl.iat[i, j]\n",
    "                if pd.notna(val) and n >= min_count:\n",
    "                    ax.text(\n",
    "                        j, i,\n",
    "                        f\"{val:.2f}\\n n={int(n)}\",\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        fontsize=7, color=\"black\",\n",
    "                    )\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    tables = {\n",
    "        \"value\": masked_vals,\n",
    "        \"count\": cnt_tbl,\n",
    "        \"x1_bins\": e1,\n",
    "        \"x2_bins\": e2,\n",
    "    }\n",
    "    return fig, ax, tables\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Example (remove in prod):\n",
    "if __name__ == \"__main__\":\n",
    "    rng = np.random.default_rng(0)\n",
    "    n = 10000\n",
    "    x1 = rng.normal(0, 1, n)\n",
    "    x2 = rng.normal(0, 1, n)\n",
    "    # fake model errors: larger when both features are large (interaction)\n",
    "    err = rng.normal(0, 0.05, n) + 0.08 * (np.maximum(0, x1) * np.maximum(0, x2))\n",
    "\n",
    "    fig, ax, tables = error_heatmap(\n",
    "        err=err,\n",
    "        x1=x1,\n",
    "        x2=x2,\n",
    "        bins=(10, 10),\n",
    "        quantile_bins=True,\n",
    "        min_count=40,\n",
    "        agg=\"mae\",\n",
    "        title=\"Error heatmap by x1 × x2 (quantile bins)\",\n",
    "        x1_name=\"feature_A\",\n",
    "        x2_name=\"feature_B\",\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f794dd6e",
   "metadata": {},
   "source": [
    "### Which features are important and why "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef294b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- requirements ---\n",
    "# pip install shap lightgbm PyALE  # (PyALE only if you later want ALE; not used here)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from typing import Optional, Sequence, Tuple, Union, Callable\n",
    "\n",
    "# ========== CORE HELPERS ==========\n",
    "\n",
    "def compute_shap_values(\n",
    "    model,\n",
    "    X: pd.DataFrame,\n",
    "    shap_values: Optional[np.ndarray] = None,\n",
    "    approximate: bool = False,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return SHAP values for a regression model.\n",
    "    If shap_values are provided, they are just returned (validated).\n",
    "    \"\"\"\n",
    "    if shap_values is not None:\n",
    "        sv = np.asarray(shap_values)\n",
    "        # for multiclass shap returns list -> not supported here\n",
    "        if isinstance(shap_values, list):\n",
    "            raise ValueError(\"Provide regression SHAP values of shape (n, m), not a list.\")\n",
    "        if sv.shape[0] != len(X) or sv.shape[1] != X.shape[1]:\n",
    "            raise ValueError(\"shap_values must have shape (n_samples, n_features).\")\n",
    "        return sv\n",
    "\n",
    "    # compute\n",
    "    explainer = shap.TreeExplainer(model) if hasattr(model, \"predict_proba\") or \"lightgbm\" in str(type(model)).lower() or \"xgb\" in str(type(model)).lower() or \"catboost\" in str(type(model)).lower() \\\n",
    "        else shap.Explainer(model, X, algorithm=\"permutation\" if approximate else None)\n",
    "\n",
    "    sv = explainer(X).values if hasattr(explainer(X), \"values\") else explainer.shap_values(X)\n",
    "    # TreeExplainer for regression returns (n, m)\n",
    "    if isinstance(sv, list):\n",
    "        raise ValueError(\"Got list of SHAP arrays (likely multiclass). Use a regression model or pass precomputed array.\")\n",
    "    return sv\n",
    "\n",
    "\n",
    "def ensure_df(X) -> pd.DataFrame:\n",
    "    return X if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n",
    "\n",
    "\n",
    "def abs_error(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "    return np.abs(np.asarray(y_true).ravel() - np.asarray(y_pred).ravel())\n",
    "\n",
    "\n",
    "# ========== PLOTS ==========\n",
    "\n",
    "def plot_shap_beeswarm(shap_values: np.ndarray, X: pd.DataFrame, top_n: int = 10, title: str = \"SHAP summary\"):\n",
    "    shap.summary_plot(shap_values, X, plot_type=\"dot\", max_display=top_n, show=False)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_shap_vs_error(\n",
    "    shap_values: np.ndarray,\n",
    "    errors: np.ndarray,\n",
    "    X: pd.DataFrame,\n",
    "    top_n: int = 10,\n",
    "    title: str = \"SHAP value vs Error\",\n",
    "):\n",
    "    \"\"\"\n",
    "    For each of the top_n features by mean|shap|, scatter SHAP value (x) vs error (color).\n",
    "    \"\"\"\n",
    "    X = ensure_df(X)\n",
    "    n, m = shap_values.shape\n",
    "    feat_order = np.argsort(np.mean(np.abs(shap_values), axis=0))[::-1][:top_n]\n",
    "    fig, ax = plt.subplots(figsize=(12, 7), dpi=150)\n",
    "    y_ticks = []\n",
    "    y_pos = 0\n",
    "    for j in feat_order:\n",
    "        sv = shap_values[:, j]\n",
    "        # offset each feature cloud vertically\n",
    "        y_vals = np.full_like(sv, fill_value=y_pos, dtype=float)\n",
    "        sc = ax.scatter(sv, y_vals, c=errors, s=8, alpha=0.7)\n",
    "        y_ticks.append(X.columns[j])\n",
    "        # dashed separator\n",
    "        ax.axhline(y_pos + 0.5, color=\"k\", linestyle=\"--\", linewidth=0.6, alpha=0.4)\n",
    "        y_pos += 1\n",
    "    cbar = plt.colorbar(sc, ax=ax)\n",
    "    cbar.set_label(\"Absolute error\")\n",
    "    ax.set_yticks(range(len(y_ticks)))\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "    ax.set_xlabel(\"SHAP value (impact on model output)\")\n",
    "    ax.set_title(title)\n",
    "    plt.axvline(0, color=\"k\", linestyle=\":\", linewidth=0.8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_feature_vs_error(\n",
    "    X: pd.DataFrame,\n",
    "    errors: np.ndarray,\n",
    "    top_n: int = 10,\n",
    "    title: str = \"Feature value vs Error\",\n",
    "    normalise: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Scatter feature value (x) vs error (color), stacked by feature.\n",
    "    \"\"\"\n",
    "    X = ensure_df(X)\n",
    "    n, m = X.shape\n",
    "    # order by variance (or later by importance you pass separately)\n",
    "    feat_order = np.argsort(X.var().values)[::-1][:top_n]\n",
    "    fig, ax = plt.subplots(figsize=(12, 7), dpi=150)\n",
    "    y_ticks = []\n",
    "    y_pos = 0\n",
    "    for j in feat_order:\n",
    "        xj = X.iloc[:, j].values.astype(float)\n",
    "        if normalise:\n",
    "            # robust center (median) and scale (IQR)\n",
    "            med = np.median(xj)\n",
    "            iqr = np.subtract(*np.percentile(xj, [75, 25])) or 1.0\n",
    "            xj = (xj - med) / iqr\n",
    "        y_vals = np.full_like(xj, fill_value=y_pos, dtype=float)\n",
    "        sc = ax.scatter(xj, y_vals, c=errors, s=8, alpha=0.7)\n",
    "        y_ticks.append(X.columns[j])\n",
    "        ax.axhline(y_pos + 0.5, color=\"k\", linestyle=\"--\", linewidth=0.6, alpha=0.4)\n",
    "        y_pos += 1\n",
    "    cbar = plt.colorbar(sc, ax=ax)\n",
    "    cbar.set_label(\"Absolute error\")\n",
    "    ax.set_yticks(range(len(y_ticks)))\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "    ax.set_xlabel(\"Feature value (robust normalised)\" if normalise else \"Feature value\")\n",
    "    ax.set_title(title)\n",
    "    plt.axvline(0, color=\"k\", linestyle=\":\", linewidth=0.8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def binned_error_curve(\n",
    "    values: np.ndarray,\n",
    "    errors: np.ndarray,\n",
    "    bins: int = 10,\n",
    "    quantile_bins: bool = True,\n",
    "    agg: str = \"mae\",\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    v = np.asarray(values).ravel()\n",
    "    e = np.asarray(errors).ravel()\n",
    "\n",
    "    if quantile_bins:\n",
    "        edges = np.unique(np.quantile(v, np.linspace(0, 1, bins + 1)))\n",
    "        if len(edges) < 3:  # fallback\n",
    "            edges = np.linspace(v.min(), v.max(), bins + 1)\n",
    "    else:\n",
    "        edges = np.linspace(v.min(), v.max(), bins + 1)\n",
    "\n",
    "    idx = np.digitize(v, edges[1:-1], right=False)\n",
    "\n",
    "    def aggfunc(arr):\n",
    "        if agg == \"mae\":\n",
    "            return np.mean(np.abs(arr))\n",
    "        if agg == \"mse\":\n",
    "            return np.mean(arr ** 2)\n",
    "        if agg == \"rmse\":\n",
    "            return np.sqrt(np.mean(arr ** 2))\n",
    "        if agg == \"mean\":\n",
    "            return np.mean(arr)\n",
    "        if agg == \"median\":\n",
    "            return np.median(np.abs(arr))\n",
    "        raise ValueError(\"Unknown agg\")\n",
    "\n",
    "    vals = np.array([aggfunc(e[idx == k]) if np.any(idx == k) else np.nan for k in range(bins)])\n",
    "    centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "    return centers[:bins], vals\n",
    "\n",
    "\n",
    "def plot_binned_error_for_top_features(\n",
    "    shap_values: np.ndarray,\n",
    "    X: pd.DataFrame,\n",
    "    errors: np.ndarray,\n",
    "    top_n: int = 6,\n",
    "    bins: int = 10,\n",
    "    quantile_bins: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    For each top feature by mean|shap|, plot two lines:\n",
    "    - error vs SHAP value bins\n",
    "    - error vs FEATURE value bins\n",
    "    \"\"\"\n",
    "    X = ensure_df(X)\n",
    "    order = np.argsort(np.mean(np.abs(shap_values), axis=0))[::-1][:top_n]\n",
    "    ncols = 3\n",
    "    nrows = int(np.ceil(top_n / ncols))\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(14, 4 * nrows), dpi=150, squeeze=False)\n",
    "\n",
    "    for i, j in enumerate(order):\n",
    "        ax = axes[i // ncols, i % ncols]\n",
    "        # SHAP bins\n",
    "        c1, y1 = binned_error_curve(shap_values[:, j], errors, bins=bins, quantile_bins=quantile_bins, agg=\"mae\")\n",
    "        # Feature bins\n",
    "        c2, y2 = binned_error_curve(X.iloc[:, j].values, errors, bins=bins, quantile_bins=quantile_bins, agg=\"mae\")\n",
    "\n",
    "        ax.plot(c1, y1, marker=\"o\", label=\"Error vs SHAP\")\n",
    "        ax.plot(c2, y2, marker=\"s\", linestyle=\"--\", label=\"Error vs Feature\")\n",
    "        ax.set_title(X.columns[j])\n",
    "        ax.set_xlabel(\"Binned value (quantiles)\" if quantile_bins else \"Binned value\")\n",
    "        ax.set_ylabel(\"MAE\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend()\n",
    "\n",
    "    # remove empty subplots\n",
    "    for k in range(i + 1, nrows * ncols):\n",
    "        fig.delaxes(axes[k // ncols, k % ncols])\n",
    "\n",
    "    fig.suptitle(\"Binned error curves (top features)\", y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def error_heatmap(\n",
    "    err: np.ndarray,\n",
    "    x1: np.ndarray,\n",
    "    x2: np.ndarray,\n",
    "    bins: Union[int, Tuple[int, int]] = (10, 10),\n",
    "    quantile_bins: bool = True,\n",
    "    min_count: int = 25,\n",
    "    agg: str = \"mae\",\n",
    "    title: str = \"Error heatmap\",\n",
    "    x1_name: str = \"x1\",\n",
    "    x2_name: str = \"x2\",\n",
    "):\n",
    "    \"\"\"\n",
    "    2D error heatmap across two features (same as we discussed earlier).\n",
    "    \"\"\"\n",
    "    if isinstance(bins, int):\n",
    "        b1 = b2 = bins\n",
    "    else:\n",
    "        b1, b2 = bins\n",
    "\n",
    "    def edges(v, k):\n",
    "        if quantile_bins:\n",
    "            e = np.unique(np.quantile(v, np.linspace(0, 1, k + 1)))\n",
    "            if len(e) < k + 1:\n",
    "                e = np.linspace(v.min(), v.max(), k + 1)\n",
    "        else:\n",
    "            e = np.linspace(v.min(), v.max(), k + 1)\n",
    "        return e\n",
    "\n",
    "    e1 = edges(x1, b1)\n",
    "    e2 = edges(x2, b2)\n",
    "\n",
    "    b1c = pd.cut(x1, e1, include_lowest=True, duplicates=\"drop\")\n",
    "    b2c = pd.cut(x2, e2, include_lowest=True, duplicate_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3f27c9",
   "metadata": {},
   "source": [
    "### Correlation / VIF between features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d937910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "feature_analysis.py\n",
    "\n",
    "A script to visualize feature correlations and compute Variance Inflation Factors (VIF).\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from patsy import dmatrix\n",
    "\n",
    "\n",
    "def plot_correlation_heatmap(df: pd.DataFrame,\n",
    "                             features: list,\n",
    "                             method: str = 'pearson',\n",
    "                             annot: bool = True,\n",
    "                             figsize: tuple = (10, 8),\n",
    "                             cmap: str = 'vlag',\n",
    "                             save_path: str = None):\n",
    "    \"\"\"\n",
    "    Plot a correlation heatmap for the specified features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame.\n",
    "    features : list\n",
    "        List of column names to include.\n",
    "    method : str\n",
    "        Correlation method: 'pearson' or 'spearman'.\n",
    "    annot : bool\n",
    "        Whether to annotate the cells with correlation coefficients.\n",
    "    figsize : tuple\n",
    "        Figure size.\n",
    "    cmap : str\n",
    "        Seaborn colormap.\n",
    "    save_path : str\n",
    "        If provided, path to save the figure (e.g. 'heatmap.png').\n",
    "    \"\"\"\n",
    "    corr = df[features].corr(method=method)\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(corr, annot=annot, cmap=cmap, center=0,\n",
    "                fmt=\".2f\", square=True, linewidths=0.5)\n",
    "    plt.title(f\"{method.capitalize()} Correlation Heatmap\")\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compute_vif(df: pd.DataFrame,\n",
    "                features: list,\n",
    "                add_constant: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute Variance Inflation Factor (VIF) for each feature.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame.\n",
    "    features : list\n",
    "        List of column names to include.\n",
    "    add_constant : bool\n",
    "        Whether to add an intercept column for VIF computation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with features and their VIFs.\n",
    "    \"\"\"\n",
    "    X = df[features].copy()\n",
    "    if add_constant:\n",
    "        # statsmodels VIF expects an intercept\n",
    "        X['Intercept'] = 1.0\n",
    "\n",
    "    vif_data = []\n",
    "    for i, col in enumerate(X.columns):\n",
    "        vif = variance_inflation_factor(X.values, i)\n",
    "        vif_data.append({'feature': col, 'VIF': vif})\n",
    "\n",
    "    vif_df = pd.DataFrame(vif_data)\n",
    "    # drop the intercept row if added\n",
    "    if add_constant:\n",
    "        vif_df = vif_df[vif_df['feature'] != 'Intercept'].reset_index(drop=True)\n",
    "    return vif_df.sort_values(by='VIF', ascending=False)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # === Example: load the Iris dataset ===\n",
    "    from sklearn.datasets import load_iris\n",
    "    iris = load_iris(as_frame=True)\n",
    "    df = iris.frame\n",
    "\n",
    "    # Specify which features to analyze\n",
    "    features = iris.feature_names\n",
    "\n",
    "    # 1) Correlation heatmap\n",
    "    plot_correlation_heatmap(df, features, method='pearson',\n",
    "                             save_path='iris_corr_heatmap.png')\n",
    "\n",
    "    # 2) VIF\n",
    "    vif_df = compute_vif(df, features)\n",
    "    print(\"\\nVariance Inflation Factors:\")\n",
    "    print(vif_df.to_string(index=False))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
