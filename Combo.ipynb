{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "775de9b0",
   "metadata": {},
   "source": [
    "### Decay Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a28413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_regress(x: np.ndarray, y: np.ndarray, *, prior_mean: np.ndarray,\n",
    "                  prior_covar: np.ndarray,\n",
    "                  decay_scale: float | np.ndarray = np.Inf,\n",
    "                  y_decay_scale: float | None = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Parallel rolling time-series regression with decay and a prior mean and\n",
    "    prior covariance matrix.\n",
    "    The N time-series are fit independently (in parallel), but share priors.\n",
    "    References: https://en.wikipedia.org/wiki/Ordinary_least_squares,\n",
    "                https://en.wikipedia.org/wiki/Weighted_least_squares,\n",
    "                https://en.wikipedia.org/wiki/Bayesian_linear_regression (see Posterior\n",
    "                distribution, mu_n, Precision Matrix).\n",
    "\n",
    "    The hack is to estimate the residual variance directly and to use that to\n",
    "    back out the Lambda matrix\n",
    "    The problem is that you need a beta to estimate the residual variance, so\n",
    "    we do two passes, the first time with the prior beta\n",
    "\n",
    "    :param x: K x T x N numeric array with the second dimension as time. K\n",
    "              variables. N time series\n",
    "              Note: x is not automatically augmented to contain an intercept term\n",
    "    :param y: T x N array. Dependent variable\n",
    "    :param prior_mean: K vector, prior for beta\n",
    "    :param prior_covar: K x K matrix, covariance matrix for beta estimates\n",
    "    :param decay_scale: number of time steps each to decay by e, OR a length-K vector\n",
    "                        of per-feature decay scales\n",
    "    :param y_decay_scale: optional single scalar decay for the y-side moments (sw, swy2).\n",
    "                          If None, defaults to the scalar decay or the first element of the\n",
    "                          decay vector.\n",
    "    :return: array of betas of shape K x T x N\n",
    "    \"\"\"\n",
    "    for arr in (x, y, prior_mean, prior_covar):\n",
    "        assert isinstance(arr, np.ndarray)\n",
    "        assert arr.dtype.kind in \"fdi\"\n",
    "        assert not np.any(np.isinf(arr))\n",
    "\n",
    "    assert x.ndim == 3\n",
    "    K, T, N = x.shape\n",
    "    assert y.shape == (T, N)\n",
    "    assert prior_mean.shape == (K,)\n",
    "    assert prior_covar.shape == (K, K)\n",
    "    assert qarray.is_positive_definite(prior_covar)\n",
    "\n",
    "    # --- BEGIN added logic for vector decay + optional y decay ---\n",
    "    if np.isscalar(decay_scale):\n",
    "        assert isinstance(decay_scale, Real)\n",
    "        assert qarray.gt(decay_scale, 0)\n",
    "        decay = np.exp(-1 / decay_scale)     # scalar decay (original behavior)\n",
    "        decay_vec = None                     # marks scalar path for x-side\n",
    "        decay_outer = decay                  # scalar\n",
    "    else:\n",
    "        decay_scale = np.asarray(decay_scale, dtype=DT_DOUBLE)\n",
    "        assert decay_scale.shape == (K,)\n",
    "        assert np.all(qarray.gt(decay_scale, 0))\n",
    "        decay_vec = np.exp(-1 / decay_scale)  # (K,)\n",
    "        decay = decay_vec[0]                  # fallback scalar for y-side if needed\n",
    "        if np.allclose(decay_vec, decay_vec[0]):\n",
    "            decay_outer = decay_vec[0]        # scalar fast path\n",
    "            decay_vec = None                  # treat as scalar for the loop\n",
    "        else:\n",
    "            decay_outer = np.sqrt(decay_vec[:, None] * decay_vec[None, :])  # K x K\n",
    "\n",
    "    if y_decay_scale is None:\n",
    "        y_decay = decay  # use scalar x-decay (or first element of vector) for y-side\n",
    "    else:\n",
    "        assert isinstance(y_decay_scale, Real)\n",
    "        assert qarray.gt(y_decay_scale, 0)\n",
    "        y_decay = np.exp(-1 / y_decay_scale)\n",
    "    # --- END added logic ---\n",
    "\n",
    "    prior_precision = np.linalg.inv(prior_covar)\n",
    "\n",
    "    # Weight NaN data points as zero\n",
    "    bad_x = np.isnan(x)\n",
    "    bad = bad_x.any(axis=0) | np.isnan(y)\n",
    "    w = (~bad).astype(DT_DOUBLE)\n",
    "\n",
    "    # Replace x and y with finite versions.\n",
    "    x0 = qarray.set_array(x, bad_x, 0)\n",
    "    y0 = qarray.set_array(y, bad, 0)\n",
    "\n",
    "    # Keep track of moments across time.\n",
    "    sw, swy2 = (np.zeros((N,), DT_DOUBLE) for _ in range(2))\n",
    "    swx2 = np.zeros((K, K, N), DT_DOUBLE)\n",
    "    swxy = np.zeros((K, N), DT_DOUBLE)\n",
    "\n",
    "    beta = np.empty(x.shape, DT_DOUBLE)\n",
    "\n",
    "    # template for filling in betas\n",
    "    beta_init = qarray.set_array(np.empty((N, K), DT_DOUBLE), np.s_[...], prior_mean)\n",
    "\n",
    "    def iterate_beta(beta_a: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Using the given beta, estimate the residual variance and use this to\n",
    "        compute a posterior beta from the given prior\n",
    "        :param beta_a: N x K\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        xy = swxy.T[:, None, :]     # N x 1 x K\n",
    "        xx = swx2.T                 # N x K x K\n",
    "        bt = beta_a[:, :, None]     # N x K x 1\n",
    "        btt = beta_a[:, None, :]    # N x 1 x K\n",
    "\n",
    "        # Use this beta to estimate the residual variance\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            res_var0 = (swy2 + (-2 * (xy @ bt) + btt @ xx @ bt)[:, 0, 0]) / sw\n",
    "\n",
    "        # It should always be non-negative\n",
    "        assert np.all(np.isnan(res_var0) | qarray.geq(res_var0, 0))\n",
    "        good = np.nonzero(qarray.geq(res_var0, 0))[0]\n",
    "\n",
    "        # We add a bit to prevent zero residual variance\n",
    "        # The reason is that we may have a few data points and therefore\n",
    "        # singular XX^T matrix\n",
    "        res_var = qarray.set_array(res_var0, np.isclose(res_var0, 0), 1e-5)\n",
    "\n",
    "        # Compute precision matrix and recompute the beta with prior.\n",
    "        lambda_ = res_var[:, None, None] * prior_precision   # N x K x K\n",
    "        a = swx2.T + lambda_                                 # N x K x K\n",
    "        b = swxy.T + lambda_ @ prior_mean                    # N x K\n",
    "        return qarray.set_array(beta_a, good,\n",
    "                                np.linalg.solve(a[good], b[good]))\n",
    "\n",
    "    for t in range(T):\n",
    "        # Update moments.\n",
    "        sw   = sw   * y_decay + w[t]                        # N      (changed: y_decay)\n",
    "        swy2 = swy2 * y_decay + w[t] * np.square(y0[t])     # N      (changed: y_decay)\n",
    "\n",
    "        if decay_vec is None:\n",
    "            # scalar path (original syntax)\n",
    "            swxy = swxy * decay + w[t] * y0[t] * x0[:, t]     # K x N\n",
    "            swx2 = swx2 * decay + w[t] * x0[:, t][:, None] * x0[:, t][None, :]  # K x K x N\n",
    "        else:\n",
    "            # per-feature decay for x-side terms\n",
    "            swxy = swxy * decay_vec[:, None] + w[t] * y0[t] * x0[:, t]          # K x N\n",
    "            swx2 = swx2 * decay_outer[:, :, None] + w[t] * x0[:, t][:, None] * x0[:, t][None, :]  # K x K x N\n",
    "\n",
    "        # Compute new posterior beta from old a couple of times.\n",
    "        beta0 = iterate_beta(beta_init)\n",
    "        beta1 = iterate_beta(beta0)\n",
    "        beta[:, t] = beta1.T\n",
    "\n",
    "    return beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637d611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# =============================================================================\n",
    "# deterministic utilities -----------------------------------------------------\n",
    "# =============================================================================\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# expanding-window CV ---------------------------------------------------------\n",
    "# =============================================================================\n",
    "def expanding_splits(\n",
    "    n_samples: int,\n",
    "    init_train_window: int,\n",
    "    test_window: int\n",
    "):\n",
    "    \"\"\"Yield (train_idx, val_idx) for an expanding window CV.\"\"\"\n",
    "    train_end = init_train_window\n",
    "    while train_end + test_window <= n_samples:\n",
    "        yield (\n",
    "            np.arange(0, train_end),\n",
    "            np.arange(train_end, train_end + test_window)\n",
    "        )\n",
    "        train_end += test_window\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# main optimiser --------------------------------------------------------------\n",
    "# =============================================================================\n",
    "def optimize_lgbm_regressor_cv(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    n_trials: int = 100,\n",
    "    random_state: int = 42,\n",
    "    init_train_window: int = 14,\n",
    "    test_window: int = 14,\n",
    ") -> Tuple[Dict, float, lgb.LGBMRegressor]:\n",
    "    \"\"\"\n",
    "    Returns (best_params, best_rmse, fitted_model) using a robust\n",
    "    'median-of-top-10' parameter selection strategy.\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1. Pre-compute CV folds once (same for every trial)  # ==== NEW ====\n",
    "    # -------------------------------------------------------------------------\n",
    "    folds: List[Tuple[np.ndarray, np.ndarray]] = list(\n",
    "        expanding_splits(len(X), init_train_window, test_window)\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2. Objective fed to Optuna\n",
    "    # -------------------------------------------------------------------------\n",
    "    def objective(trial: optuna.trial.Trial) -> float:\n",
    "        params = {\n",
    "            \"objective\": \"regression\",\n",
    "            \"metric\": \"rmse\",\n",
    "            \"boosting_type\": \"gbdt\",\n",
    "            \"verbosity\": -1,\n",
    "            \"random_state\": random_state,\n",
    "            # ----- search space -----\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 150),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "            \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.3, 0.8),\n",
    "            \"subsample_freq\": 1,\n",
    "            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.3, 0.8),\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 50, 300),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.01, 10.0, log=True),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.01, 10.0, log=True),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.2, log=True),\n",
    "        }\n",
    "\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "\n",
    "        fold_rmses = []\n",
    "        for tr_idx, val_idx in folds:\n",
    "            X_tr, y_tr = X.iloc[tr_idx], y.iloc[tr_idx]\n",
    "            X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "            model.fit(\n",
    "                X_tr,\n",
    "                y_tr,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                eval_metric=\"rmse\",\n",
    "                verbose=False,\n",
    "                callbacks=[lgb.early_stopping(25, verbose=False)],\n",
    "            )\n",
    "            preds = model.predict(X_val, num_iteration=model.best_iteration_)\n",
    "            rmse = mean_squared_error(y_val, preds, squared=False)\n",
    "            fold_rmses.append(rmse)\n",
    "\n",
    "        return float(np.mean(fold_rmses))\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3. Run Optuna\n",
    "    # -------------------------------------------------------------------------\n",
    "    sampler = optuna.samplers.TPESampler(seed=random_state)\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4. Robust selection: median of the top-10 trials    # ==== NEW ====\n",
    "    # -------------------------------------------------------------------------\n",
    "    def eval_cv(params: Dict) -> float:\n",
    "        \"\"\"Deterministic single-seed CV rescore using fixed folds.\"\"\"\n",
    "        params = params.copy()\n",
    "        params.update(\n",
    "            {\n",
    "                \"objective\": \"regression\",\n",
    "                \"metric\": \"rmse\",\n",
    "                \"verbosity\": -1,\n",
    "                \"random_state\": random_state,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        rmses = []\n",
    "        for tr_idx, val_idx in folds:\n",
    "            X_tr, y_tr = X.iloc[tr_idx], y.iloc[tr_idx]\n",
    "            X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "            model.fit(\n",
    "                X_tr,\n",
    "                y_tr,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                eval_metric=\"rmse\",\n",
    "                verbose=False,\n",
    "                callbacks=[lgb.early_stopping(25, verbose=False)],\n",
    "            )\n",
    "            preds = model.predict(X_val, num_iteration=model.best_iteration_)\n",
    "            rmses.append(mean_squared_error(y_val, preds, squared=False))\n",
    "        return float(np.mean(rmses))\n",
    "\n",
    "    # ---- grab top-10 trials by Optuna's internal score\n",
    "    top10 = sorted(study.trials, key=lambda t: t.value)[:10]\n",
    "\n",
    "    # ---- re-score them deterministically\n",
    "    rescored = []\n",
    "    for tr in top10:\n",
    "        rmse = eval_cv(tr.params)\n",
    "        rescored.append((rmse, tr.params))\n",
    "    rescored.sort(key=lambda x: x[0])  # lower RMSE = better\n",
    "\n",
    "    # ---- choose the *median* (rank-5) config\n",
    "    robust_rank = len(rescored) // 2  # 0-indexed, so 5th of 10\n",
    "    best_rmse, best_params = rescored[robust_rank]\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5. Fit final model on all data with robust params\n",
    "    # -------------------------------------------------------------------------\n",
    "    final_model = lgb.LGBMRegressor(**best_params)\n",
    "    final_model.fit(X, y)\n",
    "\n",
    "    # Optional: show what we picked\n",
    "    print(f\"Robust median-rank RMSE : {best_rmse:.5f}\")\n",
    "    print(\"Robust hyper-parameters :\")\n",
    "    for k, v in best_params.items():\n",
    "        print(f\"  {k:20s}: {v}\")\n",
    "\n",
    "    return best_params, best_rmse, final_model\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE USAGE ---------------------------------------------------------------\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Dummy data just to illustrate; replace with your own\n",
    "    X_demo = pd.DataFrame(\n",
    "        np.random.randn(200, 20), columns=[f\"f{i}\" for i in range(20)]\n",
    "    )\n",
    "    y_demo = pd.Series(np.random.randn(200))\n",
    "\n",
    "    best_params, best_rmse, model = optimize_lgbm_regressor_cv(\n",
    "        X_demo,\n",
    "        y_demo,\n",
    "        n_trials=50,\n",
    "        init_train_window=60,\n",
    "        test_window=20,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869198c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "financial_time_series_feature_code.py\n",
    "-------------------------------------\n",
    "A (reasonably) comprehensive collection of technical feature builders for OHLCV\n",
    "financial time series using pandas & numpy. You can pick and choose, or call\n",
    "`run_all()` to generate a wide panel.\n",
    "\n",
    "Assumptions:\n",
    "- Input DataFrame `df` has a DateTime index and at least the columns:\n",
    "  ['open', 'high', 'low', 'close', 'volume'] (case-insensitive accepted via params).\n",
    "- All functions are vectorized (no lookahead). Outputs align with input index.\n",
    "- Windows/parameters are customizable via function arguments.\n",
    "\n",
    "Usage Example\n",
    "-------------\n",
    ">>> import pandas as pd\n",
    ">>> from financial_time_series_feature_code import FeatureGenerator\n",
    ">>> df = pd.read_csv('prices.csv', parse_dates=['date'], index_col='date')\n",
    ">>> fg = FeatureGenerator(df)\n",
    ">>> features = fg.run_all()\n",
    ">>> features.tail()\n",
    "\n",
    "You can also call individual helpers like:\n",
    ">>> features = fg.add_returns().add_moving_averages().df\n",
    "\n",
    "Dependencies: pandas, numpy, scipy(optional for entropy). Install ta-lib if you\n",
    "want to extend with exotic indicators not included here.\n",
    "\n",
    "Author: (your name)\n",
    "License: MIT\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Iterable, Dict, List, Optional, Tuple\n",
    "\n",
    "try:\n",
    "    from scipy.stats import entropy\n",
    "except ImportError:\n",
    "    entropy = None  # Some features will check this\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Utility functions\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def _safe_col(df: pd.DataFrame, name: str, default: float = np.nan) -> pd.Series:\n",
    "    \"\"\"Return df[name] if exists else a Series of default values.\"\"\"\n",
    "    return df[name] if name in df.columns else pd.Series(default, index=df.index)\n",
    "\n",
    "\n",
    "def rolling_z(series: pd.Series, window: int, ddof: int = 0) -> pd.Series:\n",
    "    m = series.rolling(window).mean()\n",
    "    s = series.rolling(window).std(ddof=ddof)\n",
    "    return (series - m) / s\n",
    "\n",
    "\n",
    "def crossover_flag(fast: pd.Series, slow: pd.Series) -> pd.Series:\n",
    "    diff = fast - slow\n",
    "    return np.sign(diff).diff().fillna(0)\n",
    "\n",
    "\n",
    "def time_since_condition(cond: pd.Series, max_lookback: Optional[int] = None) -> pd.Series:\n",
    "    \"\"\"Bars since last True. cond is boolean Series. Vectorized implementation.\"\"\"\n",
    "    idx = np.where(cond.values, np.arange(len(cond)), np.nan)\n",
    "    # forward fill last true index\n",
    "    last = pd.Series(idx, index=cond.index).ffill()\n",
    "    dist = np.arange(len(cond)) - last\n",
    "    if max_lookback is not None:\n",
    "        dist = np.minimum(dist, max_lookback)\n",
    "    return pd.Series(dist, index=cond.index)\n",
    "\n",
    "\n",
    "def kurtosis(series: pd.Series, window: int) -> pd.Series:\n",
    "    return series.rolling(window).kurt()\n",
    "\n",
    "\n",
    "def skewness(series: pd.Series, window: int) -> pd.Series:\n",
    "    return series.rolling(window).skew()\n",
    "\n",
    "\n",
    "def pct_rank(series: pd.Series, window: int) -> pd.Series:\n",
    "    return series.rolling(window).apply(lambda x: x.rank().iloc[-1] / len(x), raw=False)\n",
    "\n",
    "\n",
    "def hurst_exponent(series: pd.Series, window: int) -> pd.Series:\n",
    "    \"\"\"Approximate rolling Hurst exponent using the R/S method.\n",
    "    Not super fast; use sparingly or vectorize further if needed.\"\"\"\n",
    "    def _hurst(x: np.ndarray) -> float:\n",
    "        if len(x) < 20 or np.std(x) == 0:\n",
    "            return np.nan\n",
    "        y = x - x.mean()\n",
    "        z = np.cumsum(y)\n",
    "        R = z.max() - z.min()\n",
    "        S = y.std()\n",
    "        if S == 0:\n",
    "            return np.nan\n",
    "        return np.log(R / S) / np.log(len(x))\n",
    "    return series.rolling(window).apply(lambda arr: _hurst(arr.values), raw=False)\n",
    "\n",
    "\n",
    "def approximate_entropy(series: pd.Series, window: int, m: int = 2, r: float = 0.2) -> pd.Series:\n",
    "    \"\"\"Approximate entropy (ApEn). Needs scipy for norms? We'll use numpy.\n",
    "    r is tolerance * std within the window. Implement simple version.\"\"\"\n",
    "    def _apen(x: np.ndarray, m: int, r: float) -> float:\n",
    "        N = len(x)\n",
    "        if N <= m + 1:\n",
    "            return np.nan\n",
    "        std_x = np.std(x)\n",
    "        if std_x == 0:\n",
    "            return np.nan\n",
    "        r_scaled = r * std_x\n",
    "        def _phi(m):\n",
    "            patterns = np.array([x[i:i+m] for i in range(N - m + 1)])\n",
    "            C = []\n",
    "            for p in patterns:\n",
    "                dist = np.max(np.abs(patterns - p), axis=1)\n",
    "                C.append(np.mean(dist <= r_scaled))\n",
    "            return np.mean(np.log(C + np.finfo(float).eps))\n",
    "        return _phi(m) - _phi(m + 1)\n",
    "    return series.rolling(window).apply(lambda arr: _apen(arr, m, r), raw=False)\n",
    "\n",
    "\n",
    "def rolling_entropy(series: pd.Series, window: int, bins: int = 20) -> pd.Series:\n",
    "    if entropy is None:\n",
    "        return pd.Series(np.nan, index=series.index)\n",
    "    def _roll_ent(x):\n",
    "        hist, _ = np.histogram(x, bins=bins)\n",
    "        return entropy(hist + 1e-12)\n",
    "    return series.rolling(window).apply(_roll_ent, raw=False)\n",
    "\n",
    "\n",
    "# Range-based volatility estimators ------------------------------------------------\n",
    "\n",
    "def parkinson_vol(high: pd.Series, low: pd.Series, window: int) -> pd.Series:\n",
    "    rs = (np.log(high / low)) ** 2\n",
    "    return np.sqrt(rs.rolling(window).mean() / (4 * np.log(2)))\n",
    "\n",
    "\n",
    "def garman_klass_vol(open_: pd.Series, high: pd.Series, low: pd.Series, close: pd.Series, window: int) -> pd.Series:\n",
    "    log_hl = np.log(high / low)\n",
    "    log_co = np.log(close / open_)\n",
    "    rs = 0.5 * (log_hl ** 2) - (2 * np.log(2) - 1) * (log_co ** 2)\n",
    "    return np.sqrt(rs.rolling(window).mean())\n",
    "\n",
    "\n",
    "def rogers_satchell_vol(open_, high, low, close, window: int) -> pd.Series:\n",
    "    rs = np.log(high / close) * np.log(high / open_) + np.log(low / close) * np.log(low / open_)\n",
    "    return np.sqrt(rs.rolling(window).mean())\n",
    "\n",
    "\n",
    "def yang_zhang_vol(open_, high, low, close, prev_close, window: int) -> pd.Series:\n",
    "    log_oc = np.log(open_ / prev_close)\n",
    "    log_co = np.log(close / open_)\n",
    "    log_hl = np.log(high / low)\n",
    "    # Using coefficients from original paper\n",
    "    k = 0.34 / (1.34 + (window + 1) / (window - 1))\n",
    "    vol_open = log_oc.rolling(window).var()\n",
    "    vol_close = log_co.rolling(window).var()\n",
    "    vol_range = log_hl.rolling(window).mean() * (np.pi ** 2 / 8)\n",
    "    return np.sqrt(vol_open + k * vol_close + (1 - k) * vol_range)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Core Feature Generator\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class FeatureGenerator:\n",
    "    df: pd.DataFrame\n",
    "    open_col: str = 'open'\n",
    "    high_col: str = 'high'\n",
    "    low_col: str = 'low'\n",
    "    close_col: str = 'close'\n",
    "    volume_col: str = 'volume'\n",
    "    # internal\n",
    "    _features_added: List[str] = field(default_factory=list)\n",
    "\n",
    "    @property\n",
    "    def px(self) -> pd.Series:\n",
    "        return self.df[self.close_col]\n",
    "\n",
    "    def _register(self, cols: Iterable[str]):\n",
    "        self._features_added.extend(cols)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Basic price/return features\n",
    "    # ------------------------------------------------------------------\n",
    "    def add_returns(self, periods: Iterable[int] = (1, 5, 10), log: bool = True) -> 'FeatureGenerator':\n",
    "        close = self.px\n",
    "        for p in periods:\n",
    "            if log:\n",
    "                self.df[f'ret_log_{p}'] = np.log(close).diff(p)\n",
    "            self.df[f'ret_{p}'] = close.pct_change(p)\n",
    "        self._register([c for c in self.df.columns if c.startswith('ret_')])\n",
    "        return self\n",
    "\n",
    "    def add_ohlc_gaps_ranges(self) -> 'FeatureGenerator':\n",
    "        o = self.df[self.open_col]\n",
    "        h = self.df[self.high_col]\n",
    "        l = self.df[self.low_col]\n",
    "        c = self.df[self.close_col]\n",
    "        self.df['hl_range'] = (h - l) / l\n",
    "        self.df['co_return'] = (c - o) / o\n",
    "        self.df['gap_pc'] = (o - c.shift(1)) / c.shift(1)\n",
    "        self.df['gap_filled'] = ((c >= c.shift(1)) & (o < c.shift(1))) | ((c <= c.shift(1)) & (o > c.shift(1)))\n",
    "        self.df['time_to_fill_gap'] = time_since_condition(self.df['gap_filled'])  # bars since filled\n",
    "        self._register(['hl_range', 'co_return', 'gap_pc', 'gap_filled', 'time_to_fill_gap'])\n",
    "        return self\n",
    "\n",
    "    def add_rolling_stats(self, windows: Iterable[int] = (5, 20, 60)) -> 'FeatureGenerator':\n",
    "        r = self.df['ret_log_1'] if 'ret_log_1' in self.df.columns else np.log(self.px).diff()\n",
    "        for w in windows:\n",
    "            self.df[f'roll_mean_ret_{w}'] = r.rolling(w).mean()\n",
    "            self.df[f'roll_std_ret_{w}'] = r.rolling(w).std()\n",
    "            self.df[f'roll_skew_ret_{w}'] = r.rolling(w).skew()\n",
    "            self.df[f'roll_kurt_ret_{w}'] = r.rolling(w).kurt()\n",
    "            self.df[f'roll_autocorr_ret_{w}'] = r.rolling(w).apply(lambda x: pd.Series(x).autocorr(lag=1), raw=False)\n",
    "        self._register([c for c in self.df.columns if c.startswith('roll_')])\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Trend / Moving averages / Bands\n",
    "    # ------------------------------------------------------------------\n",
    "    def add_moving_averages(self, fast: int = 12, slow: int = 26, other_windows: Iterable[int] = (5, 20, 60)) -> 'FeatureGenerator':\n",
    "        px = self.px\n",
    "        self.df[f'ema_{fast}'] = px.ewm(span=fast, adjust=False).mean()\n",
    "        self.df[f'ema_{slow}'] = px.ewm(span=slow, adjust=False).mean()\n",
    "        self.df['ema_diff'] = self.df[f'ema_{fast}'] - self.df[f'ema_{slow}']\n",
    "        self.df['ema_cross_flag'] = crossover_flag(self.df[f'ema_{fast}'], self.df[f'ema_{slow}'])\n",
    "        for w in other_windows:\n",
    "            self.df[f'sma_{w}'] = px.rolling(w).mean()\n",
    "        self._register(['ema_diff', 'ema_cross_flag'] + [f'ema_{fast}', f'ema_{slow}'] + [f'sma_{w}' for w in other_windows])\n",
    "        return self\n",
    "\n",
    "    def add_bollinger_bands(self, window: int = 20, n_std: float = 2.0) -> 'FeatureGenerator':\n",
    "        px = self.px\n",
    "        sma = px.rolling(window).mean()\n",
    "        std = px.rolling(window).std()\n",
    "        upper = sma + n_std * std\n",
    "        lower = sma - n_std * std\n",
    "        self.df['bb_upper'] = upper\n",
    "        self.df['bb_lower'] = lower\n",
    "        self.df['bb_pctB'] = (px - lower) / (upper - lower)\n",
    "        self.df['bb_width'] = (upper - lower) / sma\n",
    "        self._register(['bb_upper', 'bb_lower', 'bb_pctB', 'bb_width'])\n",
    "        return self\n",
    "\n",
    "    def add_donchian_channels(self, window: int = 20) -> 'FeatureGenerator':\n",
    "        h = self.df[self.high_col]\n",
    "        l = self.df[self.low_col]\n",
    "        self.df['donchian_upper'] = h.rolling(window).max()\n",
    "        self.df['donchian_lower'] = l.rolling(window).min()\n",
    "        self.df['donchian_width'] = (self.df['donchian_upper'] - self.df['donchian_lower']) / self.df['donchian_lower']\n",
    "        self._register(['donchian_upper', 'donchian_lower', 'donchian_width'])\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Momentum / Oscillators\n",
    "    # ------------------------------------------------------------------\n",
    "    def add_rsi(self, period: int = 14) -> 'FeatureGenerator':\n",
    "        px = self.px\n",
    "        delta = px.diff()\n",
    "        up = delta.clip(lower=0)\n",
    "        down = -delta.clip(upper=0)\n",
    "        roll_up = up.ewm(alpha=1/period, adjust=False).mean()\n",
    "        roll_down = down.ewm(alpha=1/period, adjust=False).mean()\n",
    "        rs = roll_up / roll_down\n",
    "        self.df[f'rsi_{period}'] = 100 - (100 / (1 + rs))\n",
    "        self._register([f'rsi_{period}'])\n",
    "        return self\n",
    "\n",
    "    def add_stochastic(self, k_period: int = 14, d_period: int = 3) -> 'FeatureGenerator':\n",
    "        h = self.df[self.high_col]\n",
    "        l = self.df[self.low_col]\n",
    "        c = self.df[self.close_col]\n",
    "        lowest_low = l.rolling(k_period).min()\n",
    "        highest_high = h.rolling(k_period).max()\n",
    "        k = 100 * (c - lowest_low) / (highest_high - lowest_low)\n",
    "        d = k.rolling(d_period).mean()\n",
    "        self.df[f'stoch_k_{k_period}'] = k\n",
    "        self.df[f'stoch_d_{d_period}'] = d\n",
    "        self._register([f'stoch_k_{k_period}', f'stoch_d_{d_period}'])\n",
    "        return self\n",
    "\n",
    "    def add_williams_r(self, period: int = 14) -> 'FeatureGenerator':\n",
    "        h = self.df[self.high_col]\n",
    "        l = self.df[self.low_col]\n",
    "        c = self.df[self.close_col]\n",
    "        highest_high = h.rolling(period).max()\n",
    "        lowest_low = l.rolling(period).min()\n",
    "        self.df[f'williamsR_{period}'] = -100 * (highest_high - c) / (highest_high - lowest_low)\n",
    "        self._register([f'williamsR_{period}'])\n",
    "        return self\n",
    "\n",
    "    def add_cci(self, period: int = 20) -> 'FeatureGenerator':\n",
    "        tp = (self.df[self.high_col] + self.df[self.low_col] + self.df[self.close_col]) / 3\n",
    "        sma = tp.rolling(period).mean()\n",
    "        mad = tp.rolling(period).apply(lambda x: np.mean(np.abs(x - x.mean())), raw=False)\n",
    "        self.df[f'cci_{period}'] = (tp - sma) / (0.015 * mad)\n",
    "        self._register([f'cci_{period}'])\n",
    "        return self\n",
    "\n",
    "    def add_macd(self, fast: int = 12, slow: int = 26, signal: int = 9) -> 'FeatureGenerator':\n",
    "        px = self.px\n",
    "        ema_fast = px.ewm(span=fast, adjust=False).mean()\n",
    "        ema_slow = px.ewm(span=slow, adjust=False).mean()\n",
    "        macd = ema_fast - ema_slow\n",
    "        sig = macd.ewm(span=signal, adjust=False).mean()\n",
    "        hist = macd - sig\n",
    "        self.df[f'macd_{fast}_{slow}'] = macd\n",
    "        self.df[f'macd_signal_{signal}'] = sig\n",
    "        self.df[f'macd_hist_{fast}_{slow}_{signal}'] = hist\n",
    "        self._register([f'macd_{fast}_{slow}', f'macd_signal_{signal}', f'macd_hist_{fast}_{slow}_{signal}'])\n",
    "        return self\n",
    "\n",
    "    def add_trix(self, period: int = 15) -> 'FeatureGenerator':\n",
    "        px = self.px\n",
    "        ema1 = px.ewm(span=period, adjust=False).mean()\n",
    "        ema2 = ema1.ewm(span=period, adjust=False).mean()\n",
    "        ema3 = ema2.ewm(span=period, adjust=False).mean()\n",
    "        self.df[f'trix_{period}'] = ema3.pct_change()\n",
    "        self._register([f'trix_{period}'])\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Volatility / Range\n",
    "    # ------------------------------------------------------------------\n",
    "    def add_volatility(self, windows: Iterable[int] = (10, 20, 60)) -> 'FeatureGenerator':\n",
    "        r = self.df['ret_log_1'] if 'ret_log_1' in self.df.columns else np.log(self.px).diff()\n",
    "        for w in windows:\n",
    "            self.df[f'realized_vol_{w}'] = r.rolling(w).std()\n",
    "        self._register([f'realized_vol_{w}' for w in windows])\n",
    "        return self\n",
    "\n",
    "    def add_atr(self, period: int = 14) -> 'FeatureGenerator':\n",
    "        h = self.df[self.high_col]\n",
    "        l = self.df[self.low_col]\n",
    "        c = self.df[self.close_col]\n",
    "        prev_c = c.shift(1)\n",
    "        tr = pd.concat([(h - l), (h - prev_c).abs(), (l - prev_c).abs()], axis=1).max(axis=1)\n",
    "        self.df[f'atr_{period}'] = tr.rolling(period).mean()\n",
    "        self._register([f'atr_{period}'])\n",
    "        return self\n",
    "\n",
    "    def add_range_vol_estimators(self, window: int = 20) -> 'FeatureGenerator':\n",
    "        o = self.df[self.open_col]\n",
    "        h = self.df[self.high_col]\n",
    "        l = self.df[self.low_col]\n",
    "        c = self.df[self.close_col]\n",
    "        prev_c = c.shift(1)\n",
    "        self.df[f'parkinson_vol_{window}'] = parkinson_vol(h, l, window)\n",
    "        self.df[f'gk_vol_{window}'] = garman_klass_vol(o, h, l, c, window)\n",
    "        self.df[f'rs_vol_{window}'] = rogers_satchell_vol(o, h, l, c, window)\n",
    "        self.df[f'yz_vol_{window}'] = yang_zhang_vol(o, h, l, c, prev_c, window)\n",
    "        self._register([f'parkinson_vol_{window}', f'gk_vol_{window}', f'rs_vol_{window}', f'yz_vol_{window}'])\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Volume / Flow\n",
    "    # ------------------------------------------------------------------\n",
    "    def add_volume_features(self, windows: Iterable[int] = (5, 20, 60)) -> 'FeatureGenerator':\n",
    "        v = self.df[self.volume_col]\n",
    "        for w in windows:\n",
    "            self.df[f'vol_mean_{w}'] = v.rolling(w).mean()\n",
    "            self.df[f'vol_z_{w}'] = rolling_z(v, w)\n",
    "            self.df[f'vol_pct_rank_{w}'] = v.rolling(w).apply(lambda x: x.rank().iloc[-1] / len(x), raw=False)\n",
    "        self._register([c for c in self.df.columns if c.startswith('vol_')])\n",
    "        return self\n",
    "\n",
    "    def add_obv(self) -> 'FeatureGenerator':\n",
    "        c = self.df[self.close_col]\n",
    "        v = self.df[self.volume_col]\n",
    "        direction = np.sign(c.diff()).fillna(0)\n",
    "        self.df['obv'] = (direction * v).cumsum()\n",
    "        self._register(['obv'])\n",
    "        return self\n",
    "\n",
    "    def add_adl_cmf_vpt(self, period: int = 20) -> 'FeatureGenerator':\n",
    "        h = self.df[self.high_col]\n",
    "        l = self.df[self.low_col]\n",
    "        c = self.df[self.close_col]\n",
    "        v = self.df[self.volume_col]\n",
    "        mfm = ((c - l) - (h - c)) / (h - l).replace(0, np.nan)  # money flow multiplier\n",
    "        mfv = mfm * v\n",
    "        self.df['adl'] = mfv.cumsum()\n",
    "        # Chaikin Money Flow\n",
    "        self.df[f'cmf_{period}'] = mfv.rolling(period).sum() / v.rolling(period).sum()\n",
    "        # Volume Price Trend\n",
    "        self.df['vpt'] = ((c.pct_change()).fillna(0) * v).cumsum()\n",
    "        self._register(['adl', f'cmf_{period}', 'vpt'])\n",
    "        return self\n",
    "\n",
    "    def add_vwap_distance(self, intraday: bool = False) -> 'FeatureGenerator':\n",
    "        \"\"\"VWAP requires intraday ticks or OHLCV with cumulative volume per day.\n",
    "        If intraday=False, we approximate rolling VWAP using daily bars (coarse).\"\"\"\n",
    "        c = self.df[self.close_col]\n",
    "        v = self.df[self.volume_col]\n",
    "        if intraday:\n",
    "            # Expect df to have a 'session' column or reset each day externally.\n",
    "            # Here we do a simple cumulative intraday VWAP per day assumption.\n",
    "            day = self.df.index.date\n",
    "            pv = (c * v).groupby(day).cumsum()\n",
    "            vv = v.groupby(day).cumsum()\n",
    "            vwap = pv / vv\n",
    "        else:\n",
    "            # Rolling VWAP over 20 bars as a fallback\n",
    "            pv = (c * v).rolling(20).sum()\n",
    "            vv = v.rolling(20).sum()\n",
    "            vwap = pv / vv\n",
    "        self.df['vwap'] = vwap\n",
    "        self.df['vwap_distance'] = (c - vwap) / vwap\n",
    "        self._register(['vwap', 'vwap_distance'])\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Cross-Asset / Relative / Correlation\n",
    "    # ------------------------------------------------------------------\n",
    "    def add_relative_strength(self, benchmark: pd.Series, windows: Iterable[int] = (20, 60)) -> 'FeatureGenerator':\n",
    "        px = self.px\n",
    "        rel = px / benchmark.reindex(px.index).ffill()\n",
    "        self.df['rel_strength'] = rel\n",
    "        for w in windows:\n",
    "            self.df[f'rel_corr_{w}'] = px.rolling(w).corr(benchmark)\n",
    "            # Beta via covariance / var\n",
    "            cov = px.pct_change().rolling(w).cov(benchmark.pct_change())\n",
    "            var = benchmark.pct_change().rolling(w).var()\n",
    "            self.df[f'beta_{w}'] = cov / var\n",
    "        self._register(['rel_strength'] + [f'rel_corr_{w}' for w in windows] + [f'beta_{w}' for w in windows])\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Seasonality / Calendar\n",
    "    # ------------------------------------------------------------------\n",
    "    def add_calendar_features(self) -> 'FeatureGenerator':\n",
    "        idx = self.df.index\n",
    "        if not isinstance(idx, pd.DatetimeIndex):\n",
    "            raise ValueError('Index must be DatetimeIndex to add calendar features')\n",
    "        self.df['dow'] = idx.dayofweek  # 0=Mon\n",
    "        self.df['dom'] = idx.day\n",
    "        self.df['month'] = idx.month\n",
    "        self.df['quarter'] = idx.quarter\n",
    "        # One-hot if needed by model\n",
    "        dow_dummies = pd.get_dummies(self.df['dow'], prefix='dow')\n",
    "        month_dummies = pd.get_dummies(self.df['month'], prefix='m')\n",
    "        self.df = pd.concat([self.df, dow_dummies, month_dummies], axis=1)\n",
    "        self._register(['dow', 'dom', 'month', 'quarter'] + list(dow_dummies.columns) + list(month_dummies.columns))\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Entropy / Fractal / Info-theoretic\n",
    "    # ------------------------------------------------------------------\n",
    "    def add_entropy_features(self, windows: Iterable[int] = (50,), bins: int = 20) -> 'FeatureGenerator':\n",
    "        r = self.df['ret_log_1'] if 'ret_log_1' in self.df.columns else np.log(self.px).diff()\n",
    "        for w in windows:\n",
    "            self.df[f'entropy_{w}'] = rolling_entropy(r, w, bins)\n",
    "        self._register([f'entropy_{w}' for w in windows])\n",
    "        return self\n",
    "\n",
    "    def add_hurst(self, windows: Iterable[int] = (100,)) -> 'FeatureGenerator':\n",
    "        r = self.df['ret_log_1'] if 'ret_log_1' in self.df.columns else np.log(self.px).diff()\n",
    "        for w in windows:\n",
    "            self.df[f'hurst_{w}'] = hurst_exponent(r, w)\n",
    "        self._register([f'hurst_{w}' for w in windows])\n",
    "        return self\n",
    "\n",
    "    def add_approx_entropy(self, windows: Iterable[int] = (100,), m: int = 2, r: float = 0.2) -> 'FeatureGenerator':\n",
    "        rts = self.df['ret_log_1'] if 'ret_log_1' in self.df.columns else np.log(self.px).diff()\n",
    "        for w in windows:\n",
    "            self.df[f'apen_{w}'] = approximate_entropy(rts, w, m=m, r=r)\n",
    "        self._register([f'apen_{w}' for w in windows])\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Event / Pattern-based (basic versions)\n",
    "    # ------------------------------------------------------------------\n",
    "    def add_breakout_flags(self, window: int = 20) -> 'FeatureGenerator':\n",
    "        h = self.df[self.high_col]\n",
    "        l = self.df[self.low_col]\n",
    "        c = self.df[self.close_col]\n",
    "        hh = h.rolling(window).max().shift(1)\n",
    "        ll = l.rolling(window).min().shift(1)\n",
    "        self.df[f'breakout_up_{window}'] = (c > hh).astype(int)\n",
    "        self.df[f'breakout_dn_{window}'] = (c < ll).astype(int)\n",
    "        self._register([f'breakout_up_{window}', f'breakout_dn_{window}'])\n",
    "        return self\n",
    "\n",
    "    def add_time_since_extrema(self, lookback: int = 60) -> 'FeatureGenerator':\n",
    "        px = self.px\n",
    "        rolling_max_idx = px.rolling(lookback).apply(lambda s: np.argmax(s), raw=False)\n",
    "        rolling_min_idx = px.rolling(lookback).apply(lambda s: np.argmin(s), raw=False)\n",
    "        self.df[f'ts_since_high_{lookback}'] = lookback - 1 - rolling_max_idx\n",
    "        self.df[f'ts_since_low_{lookback}'] = lookback - 1 - rolling_min_idx\n",
    "        self._register([f'ts_since_high_{lookback}', f'ts_since_low_{lookback}'])\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Risk / Performance Metrics (backward-looking)\n",
    "    # ------------------------------------------------------------------\n",
    "    def add_sharpe_like(self, window: int = 60) -> 'FeatureGenerator':\n",
    "        r = self.df['ret_log_1'] if 'ret_log_1' in self.df.columns else np.log(self.px).diff()\n",
    "        mean = r.rolling(window).mean()\n",
    "        vol = r.rolling(window).std()\n",
    "        self.df[f'sharpe_like_{window}'] = mean / vol\n",
    "        self._register([f'sharpe_like_{window}'])\n",
    "        return self\n",
    "\n",
    "    def add_drawdown_stats(self, window: int = 252) -> 'FeatureGenerator':\n",
    "        px = self.px\n",
    "        roll_max = px.rolling(window, min_periods=1).max()\n",
    "        dd = (px / roll_max - 1)\n",
    "        self.df[f'drawdown_{window}'] = dd\n",
    "        # Max drawdown in window\n",
    "        self.df[f'max_drawdown_{window}'] = dd.rolling(window).min()\n",
    "        self._register([f'drawdown_{window}', f'max_drawdown_{window}'])\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Convenience: Run many features\n",
    "    # ------------------------------------------------------------------\n",
    "    def run_all(self) -> pd.DataFrame:\n",
    "        \"\"\"Run a broad set. Modify to your taste. Returns the feature DataFrame.\"\"\"\n",
    "        return (self.add_returns()\n",
    "                    .add_ohlc_gaps_ranges()\n",
    "                    .add_rolling_stats()\n",
    "                    .add_moving_averages()\n",
    "                    .add_bollinger_bands()\n",
    "                    .add_donchian_channels()\n",
    "                    .add_rsi()\n",
    "                    .add_stochastic()\n",
    "                    .add_williams_r()\n",
    "                    .add_cci()\n",
    "                    .add_macd()\n",
    "                    .add_trix()\n",
    "                    .add_volatility()\n",
    "                    .add_atr()\n",
    "                    .add_range_vol_estimators()\n",
    "                    .add_volume_features()\n",
    "                    .add_obv()\n",
    "                    .add_adl_cmf_vpt()\n",
    "                    .add_vwap_distance(intraday=False)\n",
    "                    .add_calendar_features()\n",
    "                    .add_entropy_features()\n",
    "                    .add_hurst()\n",
    "                    .add_approx_entropy()\n",
    "                    .add_breakout_flags()\n",
    "                    .add_time_since_extrema()\n",
    "                    .add_sharpe_like()\n",
    "                    .add_drawdown_stats()\n",
    "                    .df)\n",
    "\n",
    "\n",
    "# If you want to run as a script for quick test\n",
    "if __name__ == \"__main__\":\n",
    "    # Quick demo with random data (for structure testing only)\n",
    "    idx = pd.date_range('2020-01-01', periods=500, freq='D')\n",
    "    rng = np.random.default_rng(42)\n",
    "    price = 100 * np.exp(np.cumsum(rng.normal(0, 0.01, size=len(idx))))\n",
    "    high = price * (1 + rng.uniform(0, 0.02, size=len(idx)))\n",
    "    low = price * (1 - rng.uniform(0, 0.02, size=len(idx)))\n",
    "    open_ = price * (1 + rng.normal(0, 0.002, size=len(idx)))\n",
    "    close = price\n",
    "    volume = rng.integers(1e5, 2e5, size=len(idx))\n",
    "    df_demo = pd.DataFrame({'open': open_, 'high': high, 'low': low, 'close': close, 'volume': volume}, index=idx)\n",
    "\n",
    "    fg = FeatureGenerator(df_demo)\n",
    "    feat = fg.run_all()\n",
    "    print(feat.tail())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
