{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7740cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def add_degree_day_features(\n",
    "    df: pd.DataFrame,\n",
    "    temp_col: str = \"temp_c\",\n",
    "    hdd_base: float = 18.0,\n",
    "    cdd_base: float = 22.0,\n",
    "    gdd_base: float = 10.0,\n",
    "    gdd_cap: float | None = 30.0,\n",
    "    prefix: str = \"\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add HDD, CDD and GDD columns to *df* and return the new DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must contain a column with outdoor temperature in °C.\n",
    "    temp_col : str\n",
    "        Name of the temperature column in *df*.\n",
    "    hdd_base : float\n",
    "        Base temperature (°C) below which heating load starts.\n",
    "    cdd_base : float\n",
    "        Base temperature (°C) above which cooling load starts.\n",
    "    gdd_base : float\n",
    "        Base temperature (°C) used in agriculture to start plant growth.\n",
    "    gdd_cap : float | None\n",
    "        Optional upper cap on temperature when computing GDD\n",
    "        (standard agronomic practice is to cap at 30 °C).\n",
    "        Use None to disable the cap.\n",
    "    prefix : str\n",
    "        Optional prefix for the new feature names if you need to\n",
    "        distinguish multiple weather stations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Original DataFrame with three extra columns:\n",
    "        • f\"{prefix}HDD{int(hdd_base)}\"\n",
    "        • f\"{prefix}CDD{int(cdd_base)}\"\n",
    "        • f\"{prefix}GDD{int(gdd_base)}\"\n",
    "    \"\"\"\n",
    "    T = df[temp_col]\n",
    "\n",
    "    # Heating Degree Days\n",
    "    df[f\"{prefix}HDD{int(hdd_base)}\"] = (hdd_base - T).clip(lower=0)\n",
    "\n",
    "    # Cooling Degree Days\n",
    "    df[f\"{prefix}CDD{int(cdd_base)}\"] = (T - cdd_base).clip(lower=0)\n",
    "\n",
    "    # Growing Degree Days\n",
    "    # 1. Apply lower base\n",
    "    gdd = (T - gdd_base).clip(lower=0)\n",
    "    # 2. Apply optional upper cap (truncated GDD)\n",
    "    if gdd_cap is not None:\n",
    "        gdd = pd.Series(\n",
    "            (T.clip(upper=gdd_cap) - gdd_base).clip(lower=0),\n",
    "            index=df.index\n",
    "        )\n",
    "    df[f\"{prefix}GDD{int(gdd_base)}\"] = gdd\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508d0713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature ramps \n",
    "df[\"T_ramp_1h\"]   = df[\"temp_c\"].diff()\n",
    "df[\"T_ramp_3h\"]   = df[\"temp_c\"].diff(3)/3      # °C per h\n",
    "df[\"T_accel_1h\"]  = df[\"T_ramp_1h\"].diff()      # second derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce731066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How volatile or unusual weather has been \n",
    "df[\"T_mean_24h\"] = df[\"temp_c\"].rolling(24).mean()\n",
    "df[\"T_std_24h\"]  = df[\"temp_c\"].rolling(24).std()\n",
    "df[\"T_range_day\"]= df[\"temp_c\"].rolling(24).max() - df[\"temp_c\"].rolling(24).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745b23f6",
   "metadata": {},
   "source": [
    "#### Curve Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5abf7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "feature_momentum_meanrev_shape.py\n",
    "---------------------------------\n",
    "Tools for electricity‑price feature engineering:\n",
    "• momentum / oscillator metrics\n",
    "• mean‑reversion diagnostics\n",
    "• intraday shape & ramp descriptors\n",
    "All formulas work with negative prices (thanks to the optional asinh transform).\n",
    "\n",
    "Author: <you> – July 2025\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# helpers\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def _safe_transform(x: pd.Series, fn):\n",
    "    \"\"\"Apply *fn* (e.g. np.arcsinh) only if it is not None.\"\"\"\n",
    "    return fn(x) if fn is not None else x\n",
    "\n",
    "\n",
    "def _ols_slope(y: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Slope of OLS line fitted to y against x = 0..n‑1.\n",
    "    Returns NaN if variance is zero (flat window).\n",
    "    \"\"\"\n",
    "    n = y.size\n",
    "    x = np.arange(n)\n",
    "    x_centered = x - x.mean()\n",
    "    y_centered = y - y.mean()\n",
    "    denom = np.dot(x_centered, x_centered)\n",
    "    if denom == 0.0:\n",
    "        return np.nan\n",
    "    return np.dot(x_centered, y_centered) / denom\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Slope / regression‑based momentum\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def rolling_slope(\n",
    "    series: pd.Series,\n",
    "    window: int = 12,\n",
    "    by_hour: bool = False,\n",
    "    transform=np.arcsinh,\n",
    "    min_periods: int | None = None,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Rolling OLS slope over *window* observations.\n",
    "    If *by_hour* is True, compute slopes separately for each hour‑of‑day.\n",
    "    \"\"\"\n",
    "    if min_periods is None:\n",
    "        min_periods = window\n",
    "    s = _safe_transform(series, transform)\n",
    "\n",
    "    if by_hour:\n",
    "        parts = []\n",
    "        for h, grp in s.groupby(series.index.hour):\n",
    "            part = grp.rolling(window, min_periods=min_periods) \\\n",
    "                      .apply(_ols_slope, raw=True)\n",
    "            parts.append(part)\n",
    "        out = pd.concat(parts).sort_index()\n",
    "    else:\n",
    "        out = s.rolling(window, min_periods=min_periods) \\\n",
    "               .apply(_ols_slope, raw=True)\n",
    "    out.name = f\"slope_{window}\"\n",
    "    return out\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Rate of Change (ROC)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def rate_of_change(\n",
    "    series: pd.Series,\n",
    "    lag: int = 1,\n",
    "    pct: bool = True,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Simple rate of change.  If *pct* is True, return % change; otherwise the\n",
    "    absolute difference.\n",
    "    \"\"\"\n",
    "    diff = series - series.shift(lag)\n",
    "    roc = diff / series.shift(lag).abs() if pct else diff\n",
    "    roc.name = f\"roc_{lag}{'pct' if pct else 'diff'}\"\n",
    "    return roc\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Relative Strength Index (RSI)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def rsi(\n",
    "    series: pd.Series,\n",
    "    window: int = 14,\n",
    "    transform=np.arcsinh,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Classic (Wilder) RSI on *transform(series)*.\n",
    "    \"\"\"\n",
    "    s = _safe_transform(series, transform)\n",
    "    delta = s.diff()\n",
    "\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = (-delta).clip(lower=0)\n",
    "\n",
    "    avg_gain = gain.rolling(window).mean()\n",
    "    avg_loss = loss.rolling(window).mean()\n",
    "\n",
    "    rs = avg_gain / avg_loss.replace(0, np.nan)\n",
    "    rsi_val = 100 - (100 / (1 + rs))\n",
    "    rsi_val.name = f\"rsi_{window}\"\n",
    "    return rsi_val\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Half‑life estimator (rolling)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def _half_life_window(y: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Half‑life τ for the AR(1) style model:\n",
    "        Δy_t = α - λ y_{t-1} + ε_t\n",
    "    τ = ln(2)/λ.  Returns NaN if λ ≤ 0 or if window too small.\n",
    "    \"\"\"\n",
    "    if y.size < 3:\n",
    "        return np.nan\n",
    "    dy = np.diff(y)\n",
    "    ylag = y[:-1]\n",
    "\n",
    "    # OLS slope of dy ~ ylag (no intercept, centred not needed here)\n",
    "    denom = np.dot(ylag, ylag)\n",
    "    if denom == 0.0:\n",
    "        return np.nan\n",
    "    beta = np.dot(ylag, dy) / denom          # beta = -λ\n",
    "    lam = -beta\n",
    "    if lam <= 0:\n",
    "        return np.nan\n",
    "    return np.log(2) / lam                   # half‑life\n",
    "\n",
    "\n",
    "def rolling_half_life(\n",
    "    series: pd.Series,\n",
    "    window: int = 168,        # one week of hourly data\n",
    "    transform=np.arcsinh,\n",
    "    min_periods: int | None = None,\n",
    ") -> pd.Series:\n",
    "    s = _safe_transform(series, transform)\n",
    "    if min_periods is None:\n",
    "        min_periods = window\n",
    "    hl = s.rolling(window, min_periods=min_periods).apply(\n",
    "        _half_life_window, raw=True\n",
    "    )\n",
    "    hl.name = f\"half_life_{window}\"\n",
    "    return hl\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5) Spike‑decay slope\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def spike_decay_slope(\n",
    "    series: pd.Series,\n",
    "    z_window: int = 24,\n",
    "    z_thresh: float = 3.0,\n",
    "    horizon: int = 6,\n",
    "    agg_window: int = 168,\n",
    "    transform=np.arcsinh,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Average slope (€/MWh per hour) seen after recent spikes.\n",
    "    For every |z|>z_thresh event, measure (P_{t+horizon} - P_t)/horizon,\n",
    "    then keep a rolling mean of those slopes over *agg_window* observations.\n",
    "    \"\"\"\n",
    "    s = _safe_transform(series, transform)\n",
    "    mu = s.rolling(z_window).mean()\n",
    "    sig = s.rolling(z_window).std()\n",
    "    z = (s - mu) / sig\n",
    "    spikes = z.abs() > z_thresh\n",
    "    decay = (s.shift(-horizon) - s) / horizon\n",
    "    decay = decay.where(spikes)\n",
    "\n",
    "    decay_avg = decay.rolling(agg_window, min_periods=1).mean()\n",
    "    decay_avg.name = f\"spike_decay_{horizon}h\"\n",
    "    return decay_avg\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6) Time‑since‑extreme (grouped by hour‑of‑day)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def time_since_extreme(\n",
    "    series: pd.Series,\n",
    "    z_window: int = 24,\n",
    "    z_thresh: float = 2.0,\n",
    "    transform=np.arcsinh,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Hours since last |z| > z_thresh *within the same hour‑of‑day*.\n",
    "    NaN until the first extreme event for that hour occurs.\n",
    "    \"\"\"\n",
    "    s = _safe_transform(series, transform)\n",
    "    mu = s.rolling(z_window).mean()\n",
    "    sig = s.rolling(z_window).std()\n",
    "    z = (s - mu) / sig\n",
    "    extreme = (z.abs() > z_thresh)\n",
    "\n",
    "    out = pd.Series(index=series.index, dtype=float)\n",
    "    last_extreme = {h: pd.NaT for h in range(24)}\n",
    "\n",
    "    for ts, flag in extreme.items():\n",
    "        h = ts.hour\n",
    "        if flag:\n",
    "            last_extreme[h] = ts\n",
    "            out.loc[ts] = 0.0\n",
    "        else:\n",
    "            if pd.isna(last_extreme[h]):\n",
    "                out.loc[ts] = np.nan\n",
    "            else:\n",
    "                delta = (ts - last_extreme[h]).total_seconds() / 3600.0\n",
    "                out.loc[ts] = delta\n",
    "    out.name = \"time_since_extreme\"\n",
    "    return out\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 7) Direct ramp metrics\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def direct_ramp_metrics(\n",
    "    series: pd.Series,\n",
    "    morning_hours: range = range(5, 8),      # 05→07\n",
    "    evening_hours: range = range(17, 21),    # 17→20\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      ramp_1h          – first difference\n",
    "      morning_ramp     – daily sum of ramp_1h across *morning_hours*\n",
    "      evening_ramp     – daily sum across *evening_hours*\n",
    "    \"\"\"\n",
    "    ramp_1h = series.diff()\n",
    "    df = pd.DataFrame({\"ramp_1h\": ramp_1h})\n",
    "\n",
    "    # aggregate within each day\n",
    "    morning_mask = series.index.hour.isin(morning_hours)\n",
    "    evening_mask = series.index.hour.isin(evening_hours)\n",
    "\n",
    "    df[\"morning_ramp\"] = (\n",
    "        ramp_1h.where(morning_mask)\n",
    "               .groupby(series.index.normalize())\n",
    "               .transform(\"sum\")\n",
    "    )\n",
    "    df[\"evening_ramp\"] = (\n",
    "        ramp_1h.where(evening_mask)\n",
    "               .groupby(series.index.normalize())\n",
    "               .transform(\"sum\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 8) Gradient & curvature of *yesterday* curve\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def gradient_curvature_prev_day(series: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    First and second difference of yesterday’s curve, aligned to today’s hours.\n",
    "    grad_prev  = P_{d-1,h} - P_{d-1,h-1}\n",
    "    curv_prev  = P_{d-1,h+1} - 2P_{d-1,h} + P_{d-1,h-1}\n",
    "    \"\"\"\n",
    "    grad_prev = series.shift(24) - series.shift(25)\n",
    "    curv_prev = series.shift(23) - 2 * series.shift(24) + series.shift(25)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"grad_prev\": grad_prev,\n",
    "        \"curv_prev\": curv_prev,\n",
    "    })\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 9) Shape against baseline (yesterday vs yesterday’s mean)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def shape_against_baseline(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    δ_{d-1,h} = P_{d-1,h} - mean(P_{d-1,·}).\n",
    "    Gives each hour’s deviation from the previous day’s daily mean.\n",
    "    \"\"\"\n",
    "    daily_mean = series.groupby(series.index.floor(\"D\")).transform(\"mean\")\n",
    "    dev = series.shift(24) - daily_mean.shift(24)\n",
    "    dev.name = \"shape_dev_prev_day\"\n",
    "    return dev"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
