{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "342cb0ea",
   "metadata": {},
   "source": [
    "#### Config.py file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7aeebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Centralised run-configuration object for the GNN project.\n",
    "(Backwards-compatible; adds optional edge-classification settings.)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import pathlib\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Any, Dict, Optional, Union, List\n",
    "\n",
    "try:\n",
    "    import yaml  # type: ignore\n",
    "except ImportError:  # pragma: no cover\n",
    "    yaml = None  # YAML support is optional\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Dataclass\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class GNNConfig:\n",
    "    # -------- task / model --------------------------------------------------\n",
    "    task: str = \"node_clf\"          # {\"node_clf\",\"node_reg\",\"edge_clf\"}  (new: \"edge_clf\")\n",
    "    model_name: str = \"gatv2\"       # {\"gcn\",\"graphsage\",\"gat\",\"gatv2\", ...}\n",
    "    num_layers: int = 2\n",
    "    hidden_dim: int = 64\n",
    "    heads: int = 4\n",
    "    dropout: float = 0.5            # layer dropout\n",
    "    norm: str = \"batch\"             # {\"batch\",\"layer\", None}\n",
    "\n",
    "    # -------- data ----------------------------------------------------------\n",
    "    target_col: str = \"target\"      # node target (unchanged)\n",
    "    # (optional) multi-target for nodes:\n",
    "    target_cols: Optional[List[str]] = None\n",
    "\n",
    "    split_mode: str = \"date\"        # {\"date\",\"ratio\"}\n",
    "    cutoff_date: Optional[str] = None\n",
    "    val_ratio: float = 0.10\n",
    "    test_ratio: float = 0.10\n",
    "    shuffle_in_split: bool = False\n",
    "\n",
    "    # -------- optimisation --------------------------------------------------\n",
    "    batch_size: int = 32\n",
    "    lr: float = 1e-3\n",
    "    epochs: int = 100\n",
    "    patience: int = 10\n",
    "    grad_clip: Optional[float] = 1.0\n",
    "\n",
    "    optimiser: str = \"adam\"         # {\"adam\",\"adamw\",\"sgd\"}\n",
    "    optimiser_kwargs: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    # -------- scheduler -----------------------------------------------------\n",
    "    lr_scheduler: Optional[str] = None      # {\"step\",\"plateau\",\"cosine\",\"onecycle\"}\n",
    "    lr_scheduler_kwargs: Dict[str, Any] = field(default_factory=dict)\n",
    "    print_lr_each_epoch: bool = True\n",
    "\n",
    "    # -------- loss / metric -------------------------------------------------\n",
    "    loss_fn: str = \"bce\"            # {\"bce\",\"focal_bce\",\"cross_entropy\",\"mse\",\"mae\",\"huber\"}\n",
    "    class_weights: Optional[Union[float, List[float], str]] = None  # node pos_weight; allow \"auto\"\n",
    "    node_pos_weights: Optional[List[float]] = None                  # per-node weighting (node tasks)\n",
    "    node_reduction: str = \"mean\"\n",
    "    metric: str = \"acc\"\n",
    "\n",
    "    # -------- regularisation -----------------------------------------------\n",
    "    in_dropout: float = 0.0\n",
    "    edge_dropout: float = 0.0\n",
    "\n",
    "    # -------- misc ----------------------------------------------------------\n",
    "    device: str = \"cuda\"\n",
    "    run_name: str = \"default_run\"\n",
    "    seed: int = 42\n",
    "\n",
    "    # =======================================================================\n",
    "    # Edge-classification additions (all optional; keep default node behavior)\n",
    "    # =======================================================================\n",
    "\n",
    "    # When task == \"edge_clf\" (or decoder is set), the trainer will use\n",
    "    # an encoder-only graph NN + an edge decoder to classify edges.\n",
    "    decoder: Optional[str] = None   # {\"dot\",\"concat_mlp\",\"hadamard_mlp\",\"bilinear\"}; None -> node tasks\n",
    "    decoder_kwargs: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    # Edge targets\n",
    "    edge_target_col: str = \"edge_target\"        # or:\n",
    "    edge_target_cols: Optional[List[str]] = None  # multi-target edges\n",
    "\n",
    "    # Edge class imbalance handling\n",
    "    edge_class_weights: Optional[Union[float, List[float], str]] = None\n",
    "    # same semantics as class_weights: None | scalar | list | \"auto\"\n",
    "    weight_smooth: float = 1.0  # optional smoothing exponent for per-edge weights\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # utils\n",
    "    # -----------------------------------------------------------------------\n",
    "    def is_edge_task(self) -> bool:\n",
    "        \"\"\"Single place to decide if we run the edge pipeline.\"\"\"\n",
    "        if isinstance(self.task, str) and self.task.lower().startswith(\"edge\"):\n",
    "            return True\n",
    "        return self.decoder is not None  # explicit override\n",
    "\n",
    "    @staticmethod\n",
    "    def load(cfg: Union[\"GNNConfig\", str, Dict[str, Any], pathlib.Path]) -> \"GNNConfig\":\n",
    "        \"\"\"\n",
    "        Flexible loader:\n",
    "\n",
    "        * existing `GNNConfig` -> returned untouched\n",
    "        * `dict` -> validated & returned\n",
    "        * `str` or `pathlib.Path` file path (YAML / JSON) -> loaded & validated\n",
    "        \"\"\"\n",
    "        if isinstance(cfg, GNNConfig):\n",
    "            return cfg\n",
    "        if isinstance(cfg, dict):\n",
    "            return GNNConfig(**cfg)\n",
    "\n",
    "        # path-like\n",
    "        path = pathlib.Path(cfg)\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(path)\n",
    "\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as fh:\n",
    "            if path.suffix.lower() in {\".yaml\", \".yml\"}:\n",
    "                if yaml is None:  # pragma: no cover\n",
    "                    raise RuntimeError(\"PyYAML not installed: `pip install pyyaml`\")\n",
    "                data = yaml.safe_load(fh)\n",
    "            elif path.suffix.lower() == \".json\":\n",
    "                data = json.load(fh)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported config file type: {path.suffix}\")\n",
    "\n",
    "        return GNNConfig(**data)\n",
    "\n",
    "    # ----------------------------- to-* helpers -----------------------------\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a deep dict (handy for logging).\"\"\"\n",
    "        return asdict(self)\n",
    "\n",
    "    def to_yaml(self) -> str:\n",
    "        \"\"\"Return a YAML string representation (requires PyYAML).\"\"\"\n",
    "        if yaml is None:  # pragma: no cover\n",
    "            raise RuntimeError(\"PyYAML not installed: `pip install pyyaml`\")\n",
    "        return yaml.safe_dump(self.to_dict(), sort_keys=False)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Convenience free-function loader\n",
    "# ---------------------------------------------------------------------------\n",
    "def load_config(cfg_like: Union[str, pathlib.Path, Dict[str, Any], GNNConfig]) -> GNNConfig:\n",
    "    \"\"\"\n",
    "    Wrapper so callers can simply write ::\n",
    "\n",
    "        cfg = load_config(\"configs/my_run.yaml\")\n",
    "\n",
    "    instead of :func:`GNNConfig.load`.\n",
    "    \"\"\"\n",
    "    return GNNConfig.load(cfg_like)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f0261",
   "metadata": {},
   "source": [
    "#### Data loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0cb531",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utilities that convert raw Pandas DataFrames -> PyTorch Geometric\n",
    "snapshots -> DataLoaders.\n",
    "\n",
    "Node tasks:\n",
    "  1) make_snapshots        : raw frames  -> List[Data] (node labels in .y)\n",
    "  2) split_snapshots       : date/ratio split\n",
    "  3) build_dataloaders     : returns (train_dl, val_dl, test_dl)\n",
    "\n",
    "Edge tasks (cfg.is_edge_task() == True):\n",
    "  1) make_edge_snapshots   : raw frames  -> List[Data] (edge labels in .edge_label)\n",
    "  2) split_snapshots       : same\n",
    "  3) build_dataloaders     : same API\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, List, Tuple, Optional, Sequence\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from jp_da_imb.gnn.config import GNNConfig\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- helpers\n",
    "\n",
    "def _edge_key(src: str, dst: str) -> str:\n",
    "    \"\"\"Normalise edge dictionary keys (case-insensitive, arrow style).\"\"\"\n",
    "    return f\"{src}->{dst}\".lower()\n",
    "\n",
    "\n",
    "def _common_timestamps(idxs: Sequence[pd.DatetimeIndex]) -> List[pd.Timestamp]:\n",
    "    \"\"\"Intersection of all DatetimeIndexes - raise if empty.\"\"\"\n",
    "    common = sorted(set.intersection(*(set(ix) for ix in idxs)))\n",
    "    if not common:\n",
    "        raise ValueError(\"No common timestamps across supplied DataFrames\")\n",
    "    return [pd.Timestamp(ts) for ts in common]\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- NODE snapshots (existing)\n",
    "\n",
    "def make_snapshots(\n",
    "    *,\n",
    "    node_frames: Dict[str, pd.DataFrame],\n",
    "    edge_frames: Dict[str, pd.DataFrame],\n",
    "    graph: nx.DiGraph,\n",
    "    cfg: GNNConfig,\n",
    ") -> List[Data]:\n",
    "    \"\"\"\n",
    "    Convert raw frames into an ordered list of `torch_geometric.data.Data`\n",
    "    snapshots for NODE tasks. Node ordering = sorted(graph.nodes).\n",
    "    \"\"\"\n",
    "    # ----- node & edge order ------------------------------------------------\n",
    "    reg_order: List[str] = sorted(graph.nodes)\n",
    "    node_pos: Dict[str, int] = {r: i for i, r in enumerate(reg_order)}\n",
    "    edge_order: List[Tuple[int, int]] = [\n",
    "        (node_pos[s], node_pos[d]) for (s, d) in graph.edges\n",
    "    ]\n",
    "\n",
    "    # ----- optional node-level weights -------------------------------------\n",
    "    if cfg.node_pos_weights is not None:\n",
    "        if len(cfg.node_pos_weights) != len(reg_order):\n",
    "            raise ValueError(\n",
    "                \"len(node_pos_weights) must equal number of nodes \"\n",
    "                f\"({len(reg_order)})\"\n",
    "            )\n",
    "        node_weight_lookup = torch.tensor(\n",
    "            cfg.node_pos_weights, dtype=torch.float32\n",
    "        )\n",
    "    else:\n",
    "        node_weight_lookup = None\n",
    "\n",
    "    # ----- decide which columns are targets --------------------------------\n",
    "    if getattr(cfg, \"target_cols\", None) is not None:\n",
    "        target_cols: List[str] = list(cfg.target_cols)  # type: ignore\n",
    "    else:\n",
    "        target_cols: List[str] = [cfg.target_col]\n",
    "\n",
    "    # ----- timestamps intersection -----------------------------------------\n",
    "    ts_common = _common_timestamps(\n",
    "        [df.index for df in node_frames.values()]\n",
    "        + [df.index for df in edge_frames.values()]\n",
    "    )\n",
    "\n",
    "    snapshots: List[Data] = []\n",
    "    for ts in ts_common:\n",
    "        # ---------- build node matrix & labels ------------------------------\n",
    "        feats: List[np.ndarray] = []\n",
    "        labels: List[np.ndarray] = []\n",
    "        for region in reg_order:\n",
    "            row = node_frames[region].loc[ts]\n",
    "            labels.append(\n",
    "                row[target_cols].to_numpy(dtype=np.float32, copy=False)\n",
    "            )\n",
    "            feats.append(\n",
    "                row.drop(labels=target_cols).to_numpy(\n",
    "                    dtype=np.float32, copy=False\n",
    "                )\n",
    "            )\n",
    "\n",
    "        x = torch.tensor(np.vstack(feats), dtype=torch.float32)\n",
    "        y = torch.tensor(np.vstack(labels), dtype=torch.float32)\n",
    "\n",
    "        # ---------- build edge features ------------------------------------\n",
    "        edge_rows: List[np.ndarray] = []\n",
    "        for s_idx, d_idx in edge_order:\n",
    "            s, d = reg_order[s_idx], reg_order[d_idx]\n",
    "            edge_rows.append(\n",
    "                edge_frames[_edge_key(s, d)]\n",
    "                .loc[ts]\n",
    "                .to_numpy(dtype=np.float32, copy=False)\n",
    "            )\n",
    "\n",
    "        edge_attr = torch.tensor(np.vstack(edge_rows), dtype=torch.float32)\n",
    "        edge_index = torch.tensor(\n",
    "            np.array(edge_order).T, dtype=torch.long\n",
    "        )\n",
    "\n",
    "        # ---------- package snapshot ---------------------------------------\n",
    "        node_weight = (\n",
    "            node_weight_lookup.clone()\n",
    "            if node_weight_lookup is not None\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        snapshots.append(\n",
    "            Data(\n",
    "                x=x,\n",
    "                edge_index=edge_index,\n",
    "                edge_attr=edge_attr,\n",
    "                y=y,\n",
    "                node_weight=node_weight,\n",
    "                snap_time=torch.tensor([ts.value]),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # sort by timestamp just in case\n",
    "    snapshots.sort(key=lambda g: g.snap_time.item())\n",
    "    return snapshots\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- EDGE snapshots (new)\n",
    "\n",
    "def _resolve_edge_target_cols(df_any: pd.DataFrame, cfg: GNNConfig) -> List[str]:\n",
    "    \"\"\"\n",
    "    Decide which columns in edge frames are the target(s).\n",
    "    Priority:\n",
    "      1) cfg.edge_target_cols (if provided and present)\n",
    "      2) cfg.edge_target_col (default \"edge_target\") if present\n",
    "      3) fallback \"target\" if present\n",
    "    \"\"\"\n",
    "    cols = list(df_any.columns)\n",
    "\n",
    "    # explicit multi-target\n",
    "    if getattr(cfg, \"edge_target_cols\", None):\n",
    "        missing = [c for c in cfg.edge_target_cols if c not in cols]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Edge target cols missing in edge frame: {missing}\")\n",
    "        return list(cfg.edge_target_cols)  # type: ignore\n",
    "\n",
    "    # single-column configured\n",
    "    if getattr(cfg, \"edge_target_col\", None) and cfg.edge_target_col in cols:\n",
    "        return [cfg.edge_target_col]  # type: ignore\n",
    "\n",
    "    # fallback common name\n",
    "    if \"target\" in cols:\n",
    "        return [\"target\"]\n",
    "\n",
    "    raise ValueError(\n",
    "        \"Could not resolve edge target column(s). \"\n",
    "        \"Set cfg.edge_target_col / edge_target_cols or include 'target'.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def make_edge_snapshots(\n",
    "    *,\n",
    "    node_frames: Dict[str, pd.DataFrame],\n",
    "    edge_frames: Dict[str, pd.DataFrame],\n",
    "    graph: nx.DiGraph,\n",
    "    cfg: GNNConfig,\n",
    ") -> List[Data]:\n",
    "    \"\"\"\n",
    "    Build snapshots for EDGE tasks.\n",
    "      - node features in .x\n",
    "      - edge_index (2,E) from graph edges\n",
    "      - edge_attr (E, F_edge) if any feature columns remain after dropping targets\n",
    "      - edge_label (E, T_edge) from configured target column(s)\n",
    "      - snap_time (int64 ns)\n",
    "    \"\"\"\n",
    "    # fixed node order & edge order\n",
    "    reg_order: List[str] = sorted(graph.nodes)\n",
    "    node_pos: Dict[str, int] = {r: i for i, r in enumerate(reg_order)}\n",
    "    edge_order: List[Tuple[int, int]] = [\n",
    "        (node_pos[s], node_pos[d]) for (s, d) in graph.edges\n",
    "    ]\n",
    "\n",
    "    # timestamps intersection across all node & edge frames\n",
    "    ts_common = _common_timestamps(\n",
    "        [df.index for df in node_frames.values()]\n",
    "        + [df.index for df in edge_frames.values()]\n",
    "    )\n",
    "\n",
    "    # infer target/feature columns once from any edge frame\n",
    "    first_edge_key = next(iter(edge_frames))\n",
    "    target_cols = _resolve_edge_target_cols(edge_frames[first_edge_key], cfg)\n",
    "    feat_cols = [c for c in edge_frames[first_edge_key].columns if c not in target_cols]\n",
    "\n",
    "    snapshots: List[Data] = []\n",
    "    for ts in ts_common:\n",
    "        # node features\n",
    "        x_rows: List[np.ndarray] = []\n",
    "        for region in reg_order:\n",
    "            row = node_frames[region].loc[ts]\n",
    "            # keep ALL node features (edge task has no node targets)\n",
    "            x_rows.append(row.to_numpy(dtype=np.float32, copy=False))\n",
    "        x = torch.tensor(np.vstack(x_rows), dtype=torch.float32)\n",
    "\n",
    "        # edge attributes and labels in same E order\n",
    "        e_feat_rows: List[np.ndarray] = []\n",
    "        e_label_rows: List[np.ndarray] = []\n",
    "        for s_idx, d_idx in edge_order:\n",
    "            s, d = reg_order[s_idx], reg_order[d_idx]\n",
    "            row = edge_frames[_edge_key(s, d)].loc[ts]\n",
    "\n",
    "            # labels\n",
    "            lab = row[target_cols].to_numpy(dtype=np.float32, copy=False)\n",
    "            e_label_rows.append(lab if lab.ndim > 0 else np.array([lab], dtype=np.float32))\n",
    "\n",
    "            # features (may be empty)\n",
    "            if feat_cols:\n",
    "                e_feat_rows.append(\n",
    "                    row[feat_cols].to_numpy(dtype=np.float32, copy=False)\n",
    "                )\n",
    "\n",
    "        edge_index = torch.tensor(np.array(edge_order).T, dtype=torch.long)\n",
    "        edge_label = torch.tensor(np.vstack(e_label_rows), dtype=torch.float32)\n",
    "        data_kwargs = dict(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            edge_label=edge_label,\n",
    "            snap_time=torch.tensor([ts.value]),\n",
    "        )\n",
    "        if feat_cols:\n",
    "            data_kwargs[\"edge_attr\"] = torch.tensor(np.vstack(e_feat_rows), dtype=torch.float32)\n",
    "\n",
    "        snapshots.append(Data(**data_kwargs))\n",
    "\n",
    "    snapshots.sort(key=lambda g: g.snap_time.item())\n",
    "    return snapshots\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- split (shared)\n",
    "\n",
    "def split_snapshots(\n",
    "    snapshots: List[Data],\n",
    "    cfg: GNNConfig,\n",
    ") -> Tuple[List[Data], List[Data], List[Data]]:\n",
    "    \"\"\"\n",
    "    Train/val/test split by **date** or **ratio** (as defined in cfg).\n",
    "    \"\"\"\n",
    "    if cfg.split_mode == \"date\":\n",
    "        if cfg.cutoff_date is None:\n",
    "            raise ValueError(\"cutoff_date must be set when split_mode == 'date'\")\n",
    "        cutoff_int = pd.Timestamp(cfg.cutoff_date).value\n",
    "        train_set = [g for g in snapshots if g.snap_time.item() <= cutoff_int]\n",
    "        holdout = [g for g in snapshots if g.snap_time.item() > cutoff_int]\n",
    "        if not holdout:\n",
    "            raise ValueError(\"No snapshots after cutoff_date for val/test split\")\n",
    "        mid = len(holdout) // 2\n",
    "        val_set, test_set = holdout[:mid], holdout[mid:]\n",
    "    else:  # ratio\n",
    "        n_total = len(snapshots)\n",
    "        n_test = int(n_total * cfg.test_ratio)\n",
    "        n_val = int(n_total * cfg.val_ratio)\n",
    "        n_train = n_total - n_val - n_test\n",
    "        train_set = snapshots[:n_train]\n",
    "        val_set = snapshots[n_train : n_train + n_val]\n",
    "        test_set = snapshots[n_train + n_val :]\n",
    "\n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- dataloaders (dispatch)\n",
    "\n",
    "def build_dataloaders(\n",
    "    *,\n",
    "    node_frames: Dict[str, pd.DataFrame],\n",
    "    edge_frames: Dict[str, pd.DataFrame],\n",
    "    graph: nx.DiGraph,\n",
    "    c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48794b1",
   "metadata": {},
   "source": [
    "#### model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8991ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Core model zoo — every network lives in this single file. Register new\n",
    "builders with the decorator from ``gnn.models.registry``.\n",
    "\n",
    "Backwards-compatible:\n",
    "- Node tasks: builders return logits by default (unchanged).\n",
    "- Edge tasks: pass as_encoder=True to get node embeddings (no final head),\n",
    "  then pair with a decoder from the small zoo below.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import (\n",
    "    GCN,\n",
    "    GraphSAGE,\n",
    "    GAT,\n",
    "    GATv2Conv,\n",
    "    GCNConv,\n",
    ")\n",
    "\n",
    "from jp_da_imb.gnn.models.registry import register\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# Encoders / Predictors\n",
    "# ======================================================================\n",
    "\n",
    "# ------------------------------------------------------------------ GCN\n",
    "\n",
    "@register(\"gcn\")\n",
    "def build_gcn(\n",
    "    *,\n",
    "    d_in: int,\n",
    "    d_out: int = 1,              # kept for signature compatibility\n",
    "    hidden_dim: int,\n",
    "    num_layers: int,\n",
    "    dropout: float,\n",
    "    norm: str = \"batch\",\n",
    "    as_encoder: bool = False,     # NEW: when True, return embeddings\n",
    "    **_,\n",
    ") -> nn.Module:\n",
    "    if as_encoder:\n",
    "        # produce hidden_dim features (no final prediction head)\n",
    "        return GCN(\n",
    "            in_channels=d_in,\n",
    "            hidden_channels=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            out_channels=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            norm=norm,\n",
    "            act=\"relu\",\n",
    "            **_,\n",
    "        )\n",
    "    else:\n",
    "        # original predictor path (unchanged)\n",
    "        return GCN(\n",
    "            in_channels=d_in,\n",
    "            hidden_channels=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            out_channels=d_out,\n",
    "            dropout=dropout,\n",
    "            norm=norm,\n",
    "            act=\"relu\",\n",
    "            **_,\n",
    "        )\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------ GraphSAGE\n",
    "\n",
    "class _GraphSageData(nn.Module):\n",
    "    \"\"\"Adapter so we can call GraphSAGE (model) on a `Data` object\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        d_in: int,\n",
    "        d_out: int,\n",
    "        hidden_dim: int,\n",
    "        num_layers: int,\n",
    "        dropout: float,\n",
    "        norm: str,\n",
    "        as_encoder: bool = False,\n",
    "        **_,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_channels = hidden_dim if as_encoder else d_out\n",
    "        self.body = GraphSAGE(\n",
    "            in_channels=d_in,\n",
    "            hidden_channels=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            out_channels=out_channels,\n",
    "            dropout=dropout,\n",
    "            norm=norm,\n",
    "            act=\"relu\",\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        return self.body(data.x, data.edge_index)\n",
    "\n",
    "\n",
    "@register(\"graphsage\")\n",
    "@register(\"sage\")  # alias\n",
    "def build_sage(**kw) -> nn.Module:\n",
    "    return _GraphSageData(**kw)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------ GAT\n",
    "\n",
    "@register(\"gat\")\n",
    "def build_gat(\n",
    "    *,\n",
    "    d_in: int,\n",
    "    d_out: int = 1,\n",
    "    hidden_dim: int,\n",
    "    num_layers: int,\n",
    "    heads: int,\n",
    "    dropout: float,\n",
    "    norm: str = \"batch\",\n",
    "    as_encoder: bool = False,\n",
    "    **_,\n",
    ") -> nn.Module:\n",
    "    out_channels = hidden_dim if as_encoder else d_out\n",
    "    return GAT(\n",
    "        in_channels=d_in,\n",
    "        hidden_channels=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        out_channels=out_channels,\n",
    "        heads=heads,\n",
    "        dropout=dropout,\n",
    "        norm=norm,\n",
    "        act=\"relu\",\n",
    "        **_,\n",
    "    )\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------ GATv2 (custom stack)\n",
    "\n",
    "class _GATv2Net(nn.Module):\n",
    "    \"\"\"\n",
    "    A flexible GATv2 implemented with `torch_geometric.nn.GATv2Conv`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d_in       : input feature dimension\n",
    "    d_out      : output dimension (e.g. 1 for binary logit) [ignored if as_encoder=True]\n",
    "    hidden_dim : hidden feature dimension inside each attention head\n",
    "    num_layers : number of GATv2Conv layers\n",
    "    heads      : number of attention heads per layer (kept constant)\n",
    "    dropout    : dropout after each layer\n",
    "    edge_dim   : optional edge-feature dimension (None -> no edge_attr)\n",
    "    norm       : \"batch\" | \"layer\" | None  – applied after each layer\n",
    "    as_encoder : if True, forward() returns node embeddings of size hidden_dim*heads\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        d_in: int,\n",
    "        d_out: int,\n",
    "        hidden_dim: int,\n",
    "        num_layers: int,\n",
    "        heads: int,\n",
    "        dropout: float,\n",
    "        edge_dim: Optional[int] = None,\n",
    "        norm: str = \"batch\",\n",
    "        as_encoder: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.as_encoder = as_encoder\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "\n",
    "        in_ch = d_in\n",
    "        for _ in range(num_layers):\n",
    "            conv = GATv2Conv(\n",
    "                in_channels=in_ch,\n",
    "                out_channels=hidden_dim,\n",
    "                heads=heads,\n",
    "                concat=True,          # keep heads concatenated\n",
    "                edge_dim=edge_dim,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            self.convs.append(conv)\n",
    "\n",
    "            if norm == \"batch\":\n",
    "                self.norms.append(nn.BatchNorm1d(hidden_dim * heads))\n",
    "            elif norm == \"layer\":\n",
    "                self.norms.append(nn.LayerNorm(hidden_dim * heads))\n",
    "            else:\n",
    "                self.norms.append(None)\n",
    "\n",
    "            in_ch = hidden_dim * heads  # next layer's input size\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # final head only used when NOT an encoder\n",
    "        self.head = nn.Linear(in_ch, d_out)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, e1 = data.x, data.edge_index\n",
    "        ea = getattr(data, \"edge_attr\", None)\n",
    "\n",
    "        for conv, norm in zip(self.convs, self.norms):\n",
    "            x = conv(x=x, edge_index=e1, edge_attr=ea)\n",
    "            if norm is not None:\n",
    "                x = norm(x)\n",
    "            x = torch.relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        if self.as_encoder:\n",
    "            return x  # embeddings [N, hidden*heads]\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "@register(\"gatv2\")\n",
    "def build_gatv2(\n",
    "    *,\n",
    "    d_in: int,\n",
    "    d_out: int = 1,\n",
    "    hidden_dim: int,\n",
    "    num_layers: int,\n",
    "    heads: int,\n",
    "    dropout: float,\n",
    "    edge_dim: Optional[int] = None,\n",
    "    norm: str = \"batch\",\n",
    "    as_encoder: bool = False,\n",
    "    **_,\n",
    ") -> nn.Module:\n",
    "    # Registry wrapper – instantiated by `build_model(...)`.\n",
    "    return _GATv2Net(\n",
    "        d_in=d_in,\n",
    "        d_out=d_out,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        heads=heads,\n",
    "        dropout=dropout,\n",
    "        edge_dim=edge_dim,\n",
    "        norm=norm,\n",
    "        as_encoder=as_encoder,\n",
    "    )\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------ Simple 2-layer baseline\n",
    "\n",
    "class _GNNNodeSimple(nn.Module):\n",
    "    \"\"\"Two GCNConv layers + linear head (predictor).\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        d_in: int,\n",
    "        d_out: int,\n",
    "        hidden_dim: int,\n",
    "        dropout: float,\n",
    "        as_encoder: bool = False,\n",
    "        **_,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.as_encoder = as_encoder\n",
    "        self.conv1 = GCNConv(in_channels=d_in, out_channels=hidden_dim)\n",
    "        self.conv2 = GCNConv(in_channels=hidden_dim, out_channels=hidden_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.head = nn.Linear(in_features=hidden_dim, out_features=d_out)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, e1 = data.x, data.edge_index\n",
    "        x = torch.relu(self.conv1(x=x, edge_index=e1))\n",
    "        x = torch.relu(self.conv2(x=x, edge_index=e1))\n",
    "        x = self.dropout(x)\n",
    "        if self.as_encoder:\n",
    "            return x\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "@register(\"simple\")\n",
    "@register(\"baseline\")\n",
    "def build_simple(\n",
    "    *,\n",
    "    d_in: int,\n",
    "    d_out: int = 1,\n",
    "    hidden_dim: int,\n",
    "    dropout: float,\n",
    "    as_encoder: bool = False,\n",
    "    **_,\n",
    ") -> nn.Module:\n",
    "    return _GNNNodeSimple(\n",
    "        d_in=d_in,\n",
    "        d_out=d_out,\n",
    "        hidden_dim=hidden_dim,\n",
    "        dropout=dropout,\n",
    "        as_encoder=as_encoder,\n",
    "    )\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# Edge decoders (z_u, z_v) -> logit\n",
    "# ======================================================================\n",
    "\n",
    "class DotDecoder(nn.Module):\n",
    "    \"\"\"z_u ⊙ z_v\"\"\"\n",
    "    def forward(self, z_src: torch.Tensor, z_dst: torch.Tensor) -> torch.Tensor:\n",
    "        return (z_src * z_dst).sum(-1, keepdim=True)\n",
    "\n",
    "\n",
    "class ConcatMLPDecoder(nn.Module):\n",
    "    \"\"\"MLP on [z_u || z_v]\"\"\"\n",
    "    def __init__(self, d_in: int, hidden: int = 128):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * d_in, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, z_src: torch.Tensor, z_dst: torch.Tensor) -> torch.Tensor:\n",
    "        return self.mlp(torch.cat([z_src, z_dst], dim=-1))\n",
    "\n",
    "\n",
    "class HadamardMLPDecoder(nn.Module):\n",
    "    \"\"\"MLP on (z_u ⊙ z_v)\"\"\"\n",
    "    def __init__(self, d_in: int, hidden: int = 128):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_in, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, z_src: torch.Tensor, z_dst: torch.Tensor) -> torch.Tensor:\n",
    "        return self.mlp(z_src * z_dst)\n",
    "\n",
    "\n",
    "class BilinearDecoder(nn.Module):\n",
    "    \"\"\"z_uᵀ W z_v\"\"\"\n",
    "    def __init__(self, d_in: int):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.empty(d_in, d_in))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "\n",
    "    def forward(self, z_src: torch.Tensor, z_dst: torch.Tensor) -> torch.Tensor:\n",
    "        return (z_src @ self.W * z_dst).sum(-1, keepdim=True)\n",
    "\n",
    "\n",
    "# Small decoder registry + builder (kept local for simplicity)\n",
    "_DECODERS = {\n",
    "    \"dot\": lambda d_in, **kw: DotDecoder(),\n",
    "    \"concat_mlp\": lambda d_in, **kw: ConcatMLPDecoder(d_in, hidden=kw.get(\"mlp_hidden\", 128)),\n",
    "    \"hadamard_mlp\": lambda d_in, **kw: HadamardMLPDecoder(d_in, hidden=kw.get(\"mlp_hidden\", 128)),\n",
    "    \"bilinear\": lambda d_in, **kw: BilinearDecoder(d_in),\n",
    "}\n",
    "\n",
    "\n",
    "def build_decoder(name: str, d_in: int, **kwargs) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Create an edge decoder by name. Example:\n",
    "        dec = build_decoder(cfg.decoder, d_in=latent_dim, mlp_hidden=128)\n",
    "    \"\"\"\n",
    "    key = name.lower()\n",
    "    try:\n",
    "        factory = _DECODERS[key]\n",
    "    except KeyError as exc:\n",
    "        raise ValueError(f\"Unknown decoder '{name}'. Available: {list(_DECODERS)}\") from exc\n",
    "    return factory(d_in, **kwargs)\n",
    "\n",
    "enc = build_model(\n",
    "    name=cfg.model_name,\n",
    "    d_in=d_in, hidden_dim=cfg.hidden_dim, num_layers=cfg.num_layers,\n",
    "    heads=cfg.heads, dropout=cfg.dropout, norm=cfg.norm,\n",
    "    as_encoder=True,              # <- new switch\n",
    "    edge_dim=edge_dim,            # if you have edge_attr and use GATv2\n",
    ")\n",
    "latent_dim = cfg.hidden_dim * (cfg.heads if cfg.model_name.lower().startswith(\"gat\") else 1)\n",
    "dec = build_decoder(cfg.decoder, d_in=latent_dim, **(cfg.decoder_kwargs or {})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e682c911",
   "metadata": {},
   "source": [
    "#### loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d258df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions that scan a DataLoader once to derive class imbalance\n",
    "statistics (for binary / multi-label classification).\n",
    "\n",
    "- compute_class_weights       : for NODE labels (batch.y)\n",
    "- compute_edge_class_weights  : for EDGE labels (batch.edge_label)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import Iterable\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "def _reduce_pos_neg(y: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    y: (..., T) with values in {0,1}\n",
    "    Returns (pos_counts[T], neg_counts[T])\n",
    "    \"\"\"\n",
    "    y = y.reshape(-1, y.size(-1)).float()\n",
    "    pos = y.sum(dim=0)\n",
    "    neg = (1.0 - y).sum(dim=0)\n",
    "    return pos, neg\n",
    "\n",
    "\n",
    "def compute_class_weights(\n",
    "    train_loader: DataLoader,\n",
    "    eps: float = 1e-6,\n",
    ") -> float | torch.Tensor:\n",
    "    \"\"\"\n",
    "    Return a `pos_weight` (scalar or per-target tensor) suitable for\n",
    "    torch.nn.functional.binary_cross_entropy_with_logits on NODE tasks.\n",
    "    \"\"\"\n",
    "    n_pos = n_neg = None\n",
    "    for batch in train_loader:\n",
    "        y = batch.y\n",
    "        pos, neg = _reduce_pos_neg(y)\n",
    "        n_pos = pos if n_pos is None else n_pos + pos\n",
    "        n_neg = neg if n_neg is None else n_neg + neg\n",
    "\n",
    "    if n_pos is None:\n",
    "        raise ValueError(\"Training loader yielded no batches\")\n",
    "\n",
    "    pos_weight = (n_neg + eps) / (n_pos + eps)\n",
    "    return pos_weight.item() if pos_weight.numel() == 1 else pos_weight\n",
    "\n",
    "\n",
    "def compute_edge_class_weights(\n",
    "    train_loader: DataLoader,\n",
    "    eps: float = 1e-6,\n",
    ") -> float | torch.Tensor:\n",
    "    \"\"\"\n",
    "    Return a `pos_weight` (scalar or per-target tensor) for EDGE tasks,\n",
    "    scanning batch.edge_label.\n",
    "    \"\"\"\n",
    "    n_pos = n_neg = None\n",
    "    for batch in train_loader:\n",
    "        y = batch.edge_label\n",
    "        # ensure shape (..., T)\n",
    "        if y.dim() == 1:\n",
    "            y = y.unsqueeze(-1)\n",
    "        pos, neg = _reduce_pos_neg(y)\n",
    "        n_pos = pos if n_pos is None else n_pos + pos\n",
    "        n_neg = neg if n_neg is None else n_neg + neg\n",
    "\n",
    "    if n_pos is None:\n",
    "        raise ValueError(\"Training loader yielded no batches\")\n",
    "\n",
    "    pos_weight = (n_neg + eps) / (n_pos + eps)\n",
    "    return pos_weight.item() if pos_weight.numel() == 1 else pos_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c41012",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Core loss zoo + registry.\n",
    "\n",
    "Each build_* returns a callable:\n",
    "\n",
    "    loss_fn(pred, target, *, sample_weight=None, node_weight=None)\n",
    "\n",
    "- Accepts either `sample_weight` (new, generic) or `node_weight` (back-compat alias)\n",
    "- Supports class-imbalance `pos_weight` for BCE-style losses\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import Any, Callable, Dict, Optional, Union, Sequence\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from jp_da_imb.gnn.loss.class_weights import (\n",
    "    compute_class_weights,\n",
    "    compute_edge_class_weights,\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------------- registry\n",
    "LOSSES: Dict[str, Callable[..., \"LossCallable\"]] = {}\n",
    "LossCallable = Callable[..., torch.Tensor]\n",
    "\n",
    "\n",
    "def register(name: str) -> Callable[[Callable[..., LossCallable]], Callable[..., LossCallable]]:\n",
    "    key = name.lower()\n",
    "\n",
    "    def _decorator(fn: Callable[..., LossCallable]) -> Callable[..., LossCallable]:\n",
    "        if key in LOSSES:\n",
    "            raise RuntimeError(f\"Loss {name} already registered\")\n",
    "        LOSSES[key] = fn\n",
    "        return fn\n",
    "\n",
    "    return _decorator\n",
    "\n",
    "\n",
    "PosWeightLike = Union[float, torch.Tensor, Sequence[float]]\n",
    "\n",
    "\n",
    "def _auto_pos_weight(train_loader: DataLoader) -> float | torch.Tensor:\n",
    "    \"\"\"\n",
    "    Detect whether the loader yields NODE batches (.y) or EDGE batches (.edge_label)\n",
    "    and compute an appropriate pos_weight tensor.\n",
    "    \"\"\"\n",
    "    first_batch = None\n",
    "    for b in train_loader:\n",
    "        first_batch = b\n",
    "        break\n",
    "    if first_batch is None:\n",
    "        raise ValueError(\"Training loader yielded no batches\")\n",
    "\n",
    "    if hasattr(first_batch, \"edge_label\"):\n",
    "        return compute_edge_class_weights(train_loader)\n",
    "    elif hasattr(first_batch, \"y\"):\n",
    "        return compute_class_weights(train_loader)\n",
    "    else:\n",
    "        raise ValueError(\"Could not find 'y' or 'edge_label' on training batches\")\n",
    "\n",
    "\n",
    "def build_loss(\n",
    "    *,\n",
    "    name: str,\n",
    "    class_weights: Optional[Union[PosWeightLike, str]] = None,\n",
    "    train_loader: Optional[DataLoader] = None,\n",
    "    node_reduction: str = \"mean\",\n",
    "    **loss_kw: Any,\n",
    ") -> LossCallable:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : registered loss id -> \"bce\", \"focal_bce\", \"mse\", ...\n",
    "    class_weights : None | scalar | list/tensor | \"auto\"\n",
    "        If \"auto\", a single pass over train_loader is made to compute pos_weight.\n",
    "    train_loader : required when class_weights == \"auto\"\n",
    "    node_reduction : \"mean\" | \"sum\" | \"none\" (applied after sample weighting)\n",
    "    **loss_kw : forwarded to the specific builder (e.g. gamma=2.0 for focal BCE)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss_fn(pred, target, *, sample_weight=None, node_weight=None) -> scalar Tensor\n",
    "    \"\"\"\n",
    "    # resolve pos_weight (BCE-only)\n",
    "    if isinstance(class_weights, str):\n",
    "        if class_weights != \"auto\":\n",
    "            raise ValueError(\"class_weights string must be 'auto' or numeric/list\")\n",
    "        if train_loader is None:\n",
    "            raise RuntimeError(\"'class_weights: auto' requires the training DataLoader\")\n",
    "        pos_weight = _auto_pos_weight(train_loader)\n",
    "    else:\n",
    "        pos_weight = class_weights  # could be None | scalar | list/tensor\n",
    "\n",
    "    # attach resolved values to kwargs expected by individual builders\n",
    "    loss_kw = {\n",
    "        **loss_kw,\n",
    "        \"pos_weight\": pos_weight,\n",
    "        \"reduction\": node_reduction,\n",
    "    }\n",
    "    if name.lower() not in {\"bce\", \"focal_bce\"}:\n",
    "        loss_kw.pop(\"pos_weight\", None)\n",
    "\n",
    "    # get builder\n",
    "    try:\n",
    "        builder = LOSSES[name.lower()]\n",
    "    except KeyError as exc:\n",
    "        raise ValueError(f\"Unknown loss '{name}'. Available: {list(LOSSES)}\") from exc\n",
    "\n",
    "    return builder(**loss_kw)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- helpers\n",
    "\n",
    "def _apply_weights(\n",
    "    loss_vec: torch.Tensor,\n",
    "    weight: Optional[torch.Tensor],\n",
    "    reduction: str,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    loss_vec: (..., N) or (..., E) or (..., N, T) — elementwise\n",
    "    weight  : broadcastable to loss_vec\n",
    "    \"\"\"\n",
    "    if weight is not None:\n",
    "        # broadcast to loss_vec shape\n",
    "        while weight.dim() < loss_vec.dim():\n",
    "            weight = weight.unsqueeze(0)\n",
    "        loss_vec = loss_vec * weight\n",
    "\n",
    "    if reduction == \"mean\":\n",
    "        return loss_vec.mean()\n",
    "    if reduction == \"sum\":\n",
    "        return loss_vec.sum()\n",
    "    return loss_vec  # \"none\"\n",
    "\n",
    "\n",
    "def _coalesce_weight(node_weight: Optional[torch.Tensor], sample_weight: Optional[torch.Tensor]) -> Optional[torch.Tensor]:\n",
    "    \"\"\"Prefer sample_weight; fall back to node_weight for back-compat.\"\"\"\n",
    "    return sample_weight if sample_weight is not None else node_weight\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- BCE\n",
    "\n",
    "@register(\"bce\")\n",
    "def build_bce(\n",
    "    *,\n",
    "    pos_weight: Optional[float | torch.Tensor] = None,\n",
    "    reduction: str = \"mean\",\n",
    ") -> LossCallable:\n",
    "    \"\"\"\n",
    "    Binary / multi-label BCE with optional global *pos_weight*.\n",
    "    Accepts sample_weight (alias: node_weight) for element-wise weighting.\n",
    "    \"\"\"\n",
    "    def _loss(\n",
    "        pred: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        *,\n",
    "        sample_weight: Optional[torch.Tensor] = None,\n",
    "        node_weight: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        w = _coalesce_weight(node_weight, sample_weight)\n",
    "        loss_vec = F.binary_cross_entropy_with_logits(\n",
    "            pred,\n",
    "            target,\n",
    "            pos_weight=(torch.as_tensor(pos_weight, device=pred.device) if pos_weight is not None else None),\n",
    "            reduction=\"none\",\n",
    "        )\n",
    "        return _apply_weights(loss_vec, w, reduction)\n",
    "\n",
    "    return _loss\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- focal BCE\n",
    "\n",
    "@register(\"focal_bce\")\n",
    "def build_focal_bce(\n",
    "    *,\n",
    "    gamma: float = 2.0,\n",
    "    alpha: float | None = None,  # class balance factor in [0,1]\n",
    "    reduction: str = \"mean\",\n",
    ") -> LossCallable:\n",
    "    \"\"\"\n",
    "    Focal BCE for imbalanced binary classification.\n",
    "    \"\"\"\n",
    "    def _loss(\n",
    "        pred: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        *,\n",
    "        sample_weight: Optional[torch.Tensor] = None,\n",
    "        node_weight: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        w = _coalesce_weight(node_weight, sample_weight)\n",
    "\n",
    "        # BCE without reduction\n",
    "        bce = F.binary_cross_entropy_with_logits(pred, target, reduction=\"none\")\n",
    "\n",
    "        # p_t = exp(-bce) is numerically stable proxy for sigmoid-anchored prob term\n",
    "        p_t = torch.exp(-bce)\n",
    "        focal_term = (1 - p_t) ** gamma\n",
    "\n",
    "        loss_vec = focal_term * bce\n",
    "        if alpha is not None:\n",
    "            alpha_t = torch.where(target == 1, torch.as_tensor(alpha, device=pred.device), 1 - torch.as_tensor(alpha, device=pred.device))\n",
    "            loss_vec = alpha_t * loss_vec\n",
    "\n",
    "        return _apply_weights(loss_vec, w, reduction)\n",
    "\n",
    "    return _loss\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- MSE\n",
    "\n",
    "@register(\"mse\")\n",
    "def build_mse(\n",
    "    *,\n",
    "    reduction: str = \"mean\",\n",
    ") -> LossCallable:\n",
    "    def _loss(\n",
    "        pred: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        *,\n",
    "        sample_weight: Optional[torch.Tensor] = None,\n",
    "        node_weight: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        w = _coalesce_weight(node_weight, sample_weight)\n",
    "        loss_vec = F.mse_loss(pred, target, reduction=\"none\")\n",
    "        return _apply_weights(loss_vec, w, reduction)\n",
    "\n",
    "    return _loss\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- MAE (L1)\n",
    "\n",
    "@register(\"mae\")\n",
    "def build_mae(\n",
    "    *,\n",
    "    reduction: str = \"mean\",\n",
    ") -> LossCallable:\n",
    "    def _loss(\n",
    "        pred: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        *,\n",
    "        sample_weight: Optional[torch.Tensor] = None,\n",
    "        node_weight: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        w = _coalesce_weight(node_weight, sample_weight)\n",
    "        loss_vec = F.l1_loss(pred, target, reduction=\"none\")\n",
    "        return _apply_weights(loss_vec, w, reduction)\n",
    "\n",
    "    return _loss\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- Huber\n",
    "\n",
    "@register(\"huber\")\n",
    "def build_huber(\n",
    "    *,\n",
    "    delta: float = 1.0,\n",
    "    reduction: str = \"mean\",\n",
    ") -> LossCallable:\n",
    "    def _loss(\n",
    "        pred: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        *,\n",
    "        sample_weight: Optional[torch.Tensor] = None,\n",
    "        node_weight: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        w = _coalesce_weight(node_weight, sample_weight)\n",
    "        loss_vec = F.huber_loss(pred, target, delta=delta, reduction=\"none\")\n",
    "        return _apply_weights(loss_vec, w, reduction)\n",
    "\n",
    "    return _loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35191c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple metric registry.\n",
    "\n",
    "All classification metrics expect LOGITS (will apply sigmoid internally).\n",
    "Regression metrics expect raw predictions.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import Callable, Dict\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, precision_score, recall_score\n",
    "\n",
    "METRICS: Dict[str, Callable[[torch.Tensor, torch.Tensor], float]] = {}\n",
    "\n",
    "\n",
    "def register(name: str):\n",
    "    def _decorator(fn):\n",
    "        METRICS[name.lower()] = fn\n",
    "        return fn\n",
    "    return _decorator\n",
    "\n",
    "\n",
    "# ---------------------- classification\n",
    "\n",
    "@register(\"auc\")\n",
    "def auroc(logits: torch.Tensor, target: torch.Tensor) -> float:\n",
    "    y_true = target.detach().cpu().numpy().ravel()\n",
    "    y_score = logits.detach().sigmoid().cpu().numpy().ravel()\n",
    "    return roc_auc_score(y_true, y_score)\n",
    "\n",
    "\n",
    "@register(\"pr_auc\")\n",
    "def pr_auc(logits: torch.Tensor, target: torch.Tensor) -> float:\n",
    "    \"\"\"Average precision (area under Precision-Recall curve).\"\"\"\n",
    "    y_true = target.detach().cpu().numpy().ravel()\n",
    "    y_score = logits.detach().sigmoid().cpu().numpy().ravel()\n",
    "    return average_precision_score(y_true, y_score)\n",
    "\n",
    "\n",
    "@register(\"acc\")\n",
    "def accuracy(logits: torch.Tensor, target: torch.Tensor) -> float:\n",
    "    pred = (logits.sigmoid() > 0.5).float()\n",
    "    return (pred == target).float().mean().item()\n",
    "\n",
    "\n",
    "@register(\"f1\")\n",
    "def f1(logits: torch.Tensor, target: torch.Tensor) -> float:\n",
    "    y_true = target.detach().cpu().numpy().ravel()\n",
    "    y_pred = (logits.detach().sigmoid().cpu().numpy().ravel() > 0.5).astype(int)\n",
    "    return f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "\n",
    "@register(\"precision\")\n",
    "def precision(logits: torch.Tensor, target: torch.Tensor) -> float:\n",
    "    y_true = target.detach().cpu().numpy().ravel()\n",
    "    y_pred = (logits.detach().sigmoid().cpu().numpy().ravel() > 0.5).astype(int)\n",
    "    return precision_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "\n",
    "@register(\"recall\")\n",
    "def recall(logits: torch.Tensor, target: torch.Tensor) -> float:\n",
    "    y_true = target.detach().cpu().numpy().ravel()\n",
    "    y_pred = (logits.detach().sigmoid().cpu().numpy().ravel() > 0.5).astype(int)\n",
    "    return recall_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "\n",
    "# ---------------------- regression\n",
    "\n",
    "@register(\"mae\")\n",
    "def mae(pred: torch.Tensor, target: torch.Tensor) -> float:\n",
    "    return torch.nn.functional.l1_loss(pred, target, reduction=\"mean\").item()\n",
    "\n",
    "\n",
    "@register(\"r2\")\n",
    "def r2(pred: torch.Tensor, target: torch.Tensor) -> float:\n",
    "    ss_res = torch.sum((target - pred) ** 2)\n",
    "    ss_tot = torch.sum((target - target.mean()) ** 2)\n",
    "    return (1 - ss_res / ss_tot).item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98668b1e",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f61ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "High-level training/evaluation loop with early stopping, schedulers,\n",
    "TensorBoard logging, and checkpointing.\n",
    "\n",
    "Backwards-compatible:\n",
    "- Node tasks: behavior unchanged.\n",
    "- Edge tasks: set cfg.task=\"edge_clf\" (or set cfg.decoder) to enable\n",
    "  encoder+decoder pipeline and train on batch.edge_label.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "from jp_da_imb.gnn.config import GNNConfig\n",
    "from jp_da_imb.gnn.models import build_model, build_decoder\n",
    "from jp_da_imb.gnn.loss.loss_functions import build_loss\n",
    "from jp_da_imb.gnn.loss.metrics import METRICS\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- small utilities\n",
    "\n",
    "def _device_from_cfg(cfg: GNNConfig) -> torch.device:\n",
    "    if cfg.device == \"cuda\" and torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def _default_log_dir(run_name: str) -> str:\n",
    "    return str(Path(\"runs\") / run_name)\n",
    "\n",
    "\n",
    "def _save_checkpoint(model: nn.Module, path: Path, meta: Dict[str, Any]) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save({\"state_dict\": model.state_dict(), \"meta\": meta}, path)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- core\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    cfg      : GNNConfig\n",
    "    train_dl : DataLoader\n",
    "    val_dl   : DataLoader\n",
    "    test_dl  : DataLoader\n",
    "    log_dir  : optional path for TensorBoard logs (default: ./runs/<run_name>)\n",
    "    ckpt_dir : optional path to save checkpoints (default: ./checkpoints)\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------- constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: GNNConfig,\n",
    "        train_dl: DataLoader,\n",
    "        val_dl: DataLoader,\n",
    "        test_dl: DataLoader,\n",
    "        *,\n",
    "        log_dir: Optional[str | Path] = None,\n",
    "        ckpt_dir: Optional[str | Path] = None,\n",
    "    ):\n",
    "        self.cfg = cfg\n",
    "        self.train_dl, self.val_dl, self.test_dl = train_dl, val_dl, test_dl\n",
    "        self.device = _device_from_cfg(cfg)\n",
    "        self.is_edge = cfg.is_edge_task()\n",
    "\n",
    "        # --- sample batch for shape inference --------------------------------\n",
    "        sample = next(iter(train_dl))\n",
    "        d_in = sample.x.size(-1)\n",
    "        edge_dim = (\n",
    "            sample.edge_attr.size(-1)\n",
    "            if getattr(sample, \"edge_attr\", None) is not None\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        # --- build model(s) ---------------------------------------------------\n",
    "        if self.is_edge:\n",
    "            # Encoder returns node embeddings; decoder maps (z_src,z_dst)->logit(s)\n",
    "            self.encoder = build_model(\n",
    "                name=cfg.model_name,\n",
    "                d_in=d_in,\n",
    "                hidden_dim=cfg.hidden_dim,\n",
    "                num_layers=cfg.num_layers,\n",
    "                heads=cfg.heads,\n",
    "                dropout=cfg.dropout,\n",
    "                norm=cfg.norm,\n",
    "                edge_dim=edge_dim,  # used by gatv2\n",
    "                as_encoder=True,    # <- key switch\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Infer latent dim robustly by a dry forward on the sample batch\n",
    "            with torch.no_grad():\n",
    "                z_probe = self.encoder(sample.to(self.device))\n",
    "            latent_dim = z_probe.size(-1)\n",
    "\n",
    "            if not cfg.decoder:\n",
    "                raise ValueError(\"Edge task requires cfg.decoder to be set (dot/concat_mlp/hadamard_mlp/bilinear).\")\n",
    "\n",
    "            self.decoder = build_decoder(\n",
    "                cfg.decoder, d_in=latent_dim, **(cfg.decoder_kwargs or {})\n",
    "            ).to(self.device)\n",
    "\n",
    "            # simple module container for optimizer\n",
    "            self.model = nn.ModuleDict({\"enc\": self.encoder, \"dec\": self.decoder}).to(self.device)\n",
    "\n",
    "            # edge targets dimension\n",
    "            t_edge = sample.edge_label.size(-1) if sample.edge_label.dim() > 1 else 1\n",
    "            self._edge_out_dim = t_edge  # currently decoders output 1; loss will broadcast if t_edge==1\n",
    "            if t_edge != 1:\n",
    "                # You can expand decoders to output multi-target; for now we enforce binary/single-target edges.\n",
    "                raise NotImplementedError(\n",
    "                    \"Current decoder outputs 1 logit per edge. \"\n",
    "                    \"Support for multi-target edge prediction (T>1) not implemented.\"\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            # Node predictor (unchanged)\n",
    "            d_out = sample.y.size(-1)\n",
    "            self.model = build_model(\n",
    "                name=cfg.model_name,\n",
    "                d_in=d_in,\n",
    "                d_out=d_out,\n",
    "                hidden_dim=cfg.hidden_dim,\n",
    "                num_layers=cfg.num_layers,\n",
    "                heads=cfg.heads,\n",
    "                dropout=cfg.dropout,\n",
    "                norm=cfg.norm,\n",
    "                edge_dim=edge_dim,  # used by gatv2\n",
    "            ).to(self.device)\n",
    "\n",
    "        # --- build loss callable ---------------------------------------------\n",
    "        # Use edge_class_weights for edge tasks, else class_weights.\n",
    "        cw = cfg.edge_class_weights if self.is_edge else cfg.class_weights\n",
    "        self.loss_fn = build_loss(\n",
    "            name=cfg.loss_fn,\n",
    "            class_weights=cw,\n",
    "            train_loader=train_dl if (isinstance(cw, str) and cw == \"auto\") else None,\n",
    "            node_reduction=self.cfg.node_reduction,\n",
    "        )\n",
    "\n",
    "        # --- build the metric -------------------------------------------------\n",
    "        self.metric = METRICS.get(cfg.metric.lower())\n",
    "        if self.metric is None:\n",
    "            raise ValueError(f\"Unknown metric: {cfg.metric}. Options: {list(METRICS)}\")\n",
    "\n",
    "        # --- optimiser --------------------------------------------------------\n",
    "        opt_kw = dict(lr=cfg.lr, **cfg.optimiser_kwargs)\n",
    "        if cfg.optimiser == \"adam\":\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), **opt_kw)\n",
    "        elif cfg.optimiser == \"adamw\":\n",
    "            self.optimizer = torch.optim.AdamW(self.model.parameters(), **opt_kw)\n",
    "        elif cfg.optimiser == \"sgd\":\n",
    "            self.optimizer = torch.optim.SGD(self.model.parameters(), **opt_kw)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimiser '{cfg.optimiser}'\")\n",
    "\n",
    "        # --- LR scheduler -----------------------------------------------------\n",
    "        self.scheduler = None\n",
    "        if cfg.lr_scheduler:\n",
    "            sname = cfg.lr_scheduler.lower()\n",
    "            kw = cfg.lr_scheduler_kwargs or {}\n",
    "            if sname == \"step\":\n",
    "                self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, **kw)\n",
    "            elif sname == \"plateau\":\n",
    "                self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, **kw)\n",
    "            elif sname == \"cosine\":\n",
    "                self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, **kw)\n",
    "            elif sname == \"onecycle\":\n",
    "                self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, **kw)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown lr_scheduler '{cfg.lr_scheduler}'\")\n",
    "\n",
    "        # --- logging ----------------------------------------------------------\n",
    "        tb_dir = _default_log_dir(cfg.run_name) if log_dir is None else str(log_dir)\n",
    "        self.writer = SummaryWriter(log_dir=tb_dir)\n",
    "        print(f\"TensorBoard logs -> {tb_dir}\")\n",
    "\n",
    "        # --- checkpointing ----------------------------------------------------\n",
    "        self.ckpt_dir = Path(ckpt_dir or \"./checkpoints\")\n",
    "        self.ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.best_val_loss = float(\"inf\")\n",
    "        self.patience_counter = 0\n",
    "\n",
    "        # history\n",
    "        self.history: Dict[str, List[float]] = dict(train=[], val=[], val_metric=[])\n",
    "\n",
    "    # ----------------------------- helpers\n",
    "\n",
    "    def _forward_logits(self, batch) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns logits for the current task.\n",
    "        - Node: model(batch) -> [N, T]\n",
    "        - Edge: decoder(z[src], z[dst]) -> [E] or [E, T]\n",
    "        \"\"\"\n",
    "        if self.is_edge:\n",
    "            z = self.encoder(batch)\n",
    "            src, dst = batch.edge_index\n",
    "            logits = self.decoder(z[src], z[dst]).squeeze(-1)  # [E] or [E, 1] -> [E]\n",
    "            return logits\n",
    "        else:\n",
    "            return self.model(batch)\n",
    "\n",
    "    def _forward_loss_and_metric(self, batch) -> Tuple[torch.Tensor, float, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns (loss_scalar, metric_value, logits_tensor)\n",
    "        \"\"\"\n",
    "        batch = batch.to(self.device)\n",
    "        logits = self._forward_logits(batch)\n",
    "\n",
    "        # targets + optional weights\n",
    "        if self.is_edge:\n",
    "            target = batch.edge_label\n",
    "            if target.dim() == 1:\n",
    "                target = target.unsqueeze(-1)  # [E, 1] to match BCE broadcasting\n",
    "            sample_weight = getattr(batch, \"edge_weight\", None)\n",
    "            loss = self.loss_fn(\n",
    "                pred=logits,\n",
    "                target=target.squeeze(-1) if logits.dim() == 1 else target,\n",
    "                sample_weight=sample_weight,\n",
    "            )\n",
    "            metric_val = self.metric(logits, target)\n",
    "        else:\n",
    "            target = batch.y\n",
    "            node_weight = getattr(batch, \"node_weight\", None)\n",
    "            loss = self.loss_fn(pred=self._maybe_squeeze(logits, target), target=target, node_weight=node_weight)\n",
    "            metric_val = self.metric(logits, target)\n",
    "\n",
    "        return loss, float(metric_val), logits\n",
    "\n",
    "    @staticmethod\n",
    "    def _maybe_squeeze(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        # convenience: if pred is [N] and target [N,1], squeeze target shape\n",
    "        if pred.dim() == 1 and target.dim() == 2 and target.size(-1) == 1:\n",
    "            return pred.unsqueeze(-1)\n",
    "        return pred\n",
    "\n",
    "    # ----------------------------- epoch funcs\n",
    "\n",
    "    def _train_epoch(self) -> float:\n",
    "        self.model.train()\n",
    "        total = 0.0\n",
    "        for batch in self.train_dl:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss, _, _ = self._forward_loss_and_metric(batch)\n",
    "            loss.backward()\n",
    "            if self.cfg.grad_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.grad_clip)\n",
    "            self.optimizer.step()\n",
    "            total += loss.item()\n",
    "        return total / max(1, len(self.train_dl))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _eval(self, loader: DataLoader) -> tuple[float, float]:\n",
    "        self.model.eval()\n",
    "        tot_loss, tot_metric, n_batches = 0.0, 0.0, 0\n",
    "        for batch in loader:\n",
    "            loss, metric_val, _ = self._forward_loss_and_metric(batch)\n",
    "            tot_loss += loss.item()\n",
    "            tot_metric += metric_val\n",
    "            n_batches += 1\n",
    "        return tot_loss / max(1, n_batches), tot_metric / max(1, n_batches)\n",
    "\n",
    "    # ----------------------------- main loop\n",
    "\n",
    "    def fit(self) -> None:\n",
    "        for epoch in range(1, self.cfg.epochs + 1):\n",
    "            train_loss = self._train_epoch()\n",
    "            val_loss, val_metric = self._eval(self.val_dl)\n",
    "\n",
    "            # scheduler step\n",
    "            if self.scheduler is not None:\n",
    "                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    self.scheduler.step(val_loss)\n",
    "                else:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # tensorboard\n",
    "            if self.writer:\n",
    "                self.writer.add_scalar(\"loss/train\", train_loss, epoch)\n",
    "                self.writer.add_scalar(\"loss/val\", val_loss, epoch)\n",
    "                self.writer.add_scalar(f\"{self.cfg.metric}/val\", val_metric, epoch)\n",
    "\n",
    "            # console\n",
    "            lr = self.optimizer.param_groups[0][\"lr\"]\n",
    "            if self.cfg.print_lr_each_epoch:\n",
    "                print(\n",
    "                    f\"[{epoch:03d}/{self.cfg.epochs}] \"\n",
    "                    f\"train={train_loss:.4f}  val={val_loss:.4f}  lr={lr:.2e}  \"\n",
    "                    f\"{self.cfg.metric}(val)={val_metric:.4f}\"\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    f\"[{epoch:03d}/{self.cfg.epochs}] train={train_loss:.4f}  val={val_loss:.4f}  {self.cfg.metric}(val)={val_metric:.4f}\"\n",
    "                )\n",
    "\n",
    "            self.history[\"train\"].append(train_loss)\n",
    "            self.history[\"val\"].append(val_loss)\n",
    "            self.history.setdefault(\"val_metric\", []).append(val_metric)\n",
    "\n",
    "            # early stopping + checkpoint\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.patience_counter = 0\n",
    "                _save_checkpoint(\n",
    "                    self.model,\n",
    "                    path=self.ckpt_dir / \"best.pt\",\n",
    "                    meta=dict(cfg=self.cfg.to_dict(), epoch=epoch, val_loss=val_loss),\n",
    "                )\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                if self.patience_counter >= self.cfg.patience:\n",
    "                    print(f\"Early stopping (no val improvement in {self.cfg.patience} epochs).\")\n",
    "                    break\n",
    "\n",
    "        if self.writer:\n",
    "            self.writer.flush()\n",
    "\n",
    "    # ----------------------------- test / inference\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(\n",
    "        self,\n",
    "        split: str = \"test\",\n",
    "        *,\n",
    "        return_df: bool = False,\n",
    "    ) -> Dict[str, float] | Tuple[Dict[str, float], pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Compute loss + metric on *split*; optionally also return a tidy DataFrame.\n",
    "\n",
    "        Node DF columns:  snap_time, node, pred[0..], target[0..]\n",
    "        Edge DF columns:  snap_time, src, dst,  pred[0..], target[0..]\n",
    "        \"\"\"\n",
    "        loader = {\"train\": self.train_dl, \"val\": self.val_dl, \"test\": self.test_dl}[split]\n",
    "\n",
    "        self.model.eval()\n",
    "        total_loss = total_metric = 0.0\n",
    "        records: List[Dict[str, Any]] = []\n",
    "\n",
    "        for batch in loader:\n",
    "            loss, metric_val, logits = self._forward_loss_and_metric(batch)\n",
    "            total_loss += loss.item()\n",
    "            total_metric += metric_val\n",
    "\n",
    "            if return_df:\n",
    "                if self.is_edge:\n",
    "                    self._append_edge_records(records, batch, logits)\n",
    "                else:\n",
    "                    self._append_node_records(records, batch, logits)\n",
    "\n",
    "        stats = {\n",
    "            \"loss\": total_loss / max(1, len(loader)),\n",
    "            f\"{self.cfg.metric}\": total_metric / max(1, len(loader)),\n",
    "        }\n",
    "        if not return_df:\n",
    "            return stats\n",
    "\n",
    "        df = pd.DataFrame.from_records(records)\n",
    "        return stats, df\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, loader: Optional[DataLoader] = None) -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward-only predictions (list of tensors per batch).\n",
    "        \"\"\"\n",
    "        loader = loader or self.test_dl\n",
    "        self.model.eval()\n",
    "        out: List[torch.Tensor] = []\n",
    "        for batch in loader:\n",
    "            batch = batch.to(self.device)\n",
    "            logits = self._forward_logits(batch)\n",
    "            out.append(logits.detach().cpu())\n",
    "        return out\n",
    "\n",
    "    # ----------------------------- dataframe helpers\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_batch(batch) -> Batch:\n",
    "        # PyG sometimes hands a Data if batch_size==1; normalize to Batch API\n",
    "        return batch if isinstance(batch, Batch) else Batch.from_data_list([batch])\n",
    "\n",
    "    def _append_node_records(self, records: List[Dict[str, Any]], batch, logits: torch.Tensor) -> None:\n",
    "        batch = self._ensure_batch(batch)\n",
    "        # logits: [sum_N, T], target: [sum_N, T]\n",
    "        pred = logits.detach().cpu()\n",
    "        tgt = batch.y.detach().cpu()\n",
    "        tss = batch.snap_time.detach().cpu().numpy()  # [num_graphs]\n",
    "\n",
    "        # infer shapes\n",
    "        n_tar = pred.size(-1)\n",
    "        # number of nodes per graph can be derived from ptr diffs\n",
    "        ptr = batch.ptr.detach().cpu().numpy()  # len = num_graphs+1\n",
    "        for g in range(len(tss)):\n",
    "            start, end = ptr[g], ptr[g + 1]\n",
    "            ts = pd.Timestamp(int(batch.snap_time[g].item()))\n",
    "            for node in range(end - start):\n",
    "                rec: Dict[str, Any] = {\"snap_time\": ts, \"node\": node}\n",
    "                for t in range(n_tar):\n",
    "                    rec[\"pred\" + (str(t) if n_tar > 1 else \"\")] = float(pred[start + node, t].item())\n",
    "                    rec[\"target\" + (str(t) if n_tar > 1 else \"\")] = float(tgt[start + node, t].item())\n",
    "                records.append(rec)\n",
    "\n",
    "    def _append_edge_records(self, records: List[Dict[str, Any]], batch, logits: torch.Tensor) -> None:\n",
    "        batch = self._ensure_batch(batch)\n",
    "        # logits: [sum_E] or [sum_E, T], edge_label [sum_E] or [sum_E, T]\n",
    "        pred = logits.detach().cpu()\n",
    "        tgt = batch.edge_label.detach().cpu()\n",
    "        if tgt.dim() == 1:\n",
    "            tgt = tgt.unsqueeze(-1)\n",
    "        if pred.dim() == 1:\n",
    "            pred = pred.unsqueeze(-1)\n",
    "        n_tar = pred.size(-1)\n",
    "\n",
    "        # map edges to graphs via source node's graph id\n",
    "        src, dst = batch.edge_index\n",
    "        gids = batch.batch[src].detach().cpu().numpy()  # graph id per edge\n",
    "        ptr = batch.ptr.detach().cpu().numpy()          # node-offset per graph\n",
    "        tss = batch.snap_time.detach().cpu().numpy()    # [num_graphs]\n",
    "\n",
    "        for i in range(pred.size(0)):  # over edges\n",
    "            g = int(gids[i])\n",
    "            node_off = int(ptr[g])\n",
    "            ts = pd.Timestamp(int(tss[g]))\n",
    "            src_local = int(src[i].item() - node_off)\n",
    "            dst_local = int(dst[i].item() - node_off)\n",
    "\n",
    "            rec: Dict[str, Any] = {\"snap_time\": ts, \"src\": src_local, \"dst\": dst_local}\n",
    "            for t in range(n_tar):\n",
    "                rec[\"pred\" + (str(t) if n_tar > 1 else \"\")] = float(pred[i, t].item())\n",
    "                rec[\"target\" + (str(t) if n_tar > 1 else \"\")] = float(tgt[i, t].item())\n",
    "            records.append(rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7a378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Notes / rationale\n",
    "\n",
    "Edge vs node: a single self.is_edge flag (from cfg.is_edge_task()) drives the branch everywhere.\n",
    "\n",
    "Encoders/decoders: for edges we instantiate the node encoder (as_encoder=True), infer latent_dim by a dry forward, then build the decoder with build_decoder.\n",
    "\n",
    "Losses: build_loss now accepts \"auto\" weights for both node and edge. We feed edge_class_weights in edge mode (or fall back to class_weights if you prefer).\n",
    "\n",
    "Sample weights: wired via sample_weight= (or node_weight for node tasks). If you later add edge_weight tensors to each Data, they’ll be picked up automatically.\n",
    "\n",
    "Schedulers: supports step, plateau, cosine, onecycle (unchanged). plateau steps on val_loss.\n",
    "\n",
    "Evaluate (tidy DF):\n",
    "\n",
    "Node: outputs snap_time, node, pred*, target*.\n",
    "\n",
    "Edge: outputs snap_time, src, dst, pred*, target*. We robustly map edges to their graph via batch.batch[src], and convert global node ids to per-graph indices using batch.ptr.\n",
    "\n",
    "Multi-target edges: currently raises if T_edge > 1 (decoders return 1 logit). If you want multi-target, we can extend decoders to output out_dim=T_edge and remove that check.\n",
    "\n",
    "If you want me to also add optional per-edge smoothing weights (your separate code’s weight_smooth idea), we can add a small helper to compute and attach edge_weight tensors to each snapshot post-split—say in build_dataloaders or a compute_edge_weights(train_set) function—and the trainer will consume them automatically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
