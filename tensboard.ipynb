{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d14d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ModelRunLogger: Generic TensorBoard logging for regression models + convenience wrapper.\n",
    "\n",
    "Features:\n",
    "- Run directory management\n",
    "- Config & hyperparameter logging (as scalars + JSON)\n",
    "- Regression metrics (MSE, RMSE, MAE, R²) for train/test/etc.\n",
    "- Datetime-indexed series logging as scalars using walltime (Option 1)\n",
    "- Multi-series logging (automatic overlay via consistent tags)\n",
    "- Matplotlib figure logging\n",
    "- Optional CSV export of each logged series\n",
    "- Convenience wrapper function `log_basic_run` to avoid boilerplate\n",
    "\n",
    "Usage (simple):\n",
    "    summary = log_basic_run(\n",
    "        model=model,\n",
    "        model_name=\"zoneA_lgbm\",\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        X_test=X_test,   y_test=y_test,\n",
    "        model_params=lgbm_params,\n",
    "        pnl_series=pnl_series,\n",
    "        sharpe_series=sharpe_series,\n",
    "        extra_series={\"hit_rate\": hit_rate_series},\n",
    "        zone=\"A\",\n",
    "        target=\"da_imb_spread\",\n",
    "    )\n",
    "    print(\"Test RMSE:\", summary[\"metrics\"][\"test_final\"][\"rmse\"])\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import shutil\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Mapping\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Utility functions\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _utc_timestamp_string() -> str:\n",
    "    return dt.datetime.utcnow().strftime(\"%Y%m%d_%H%M%%S\".replace(\"%%\", \"%\"))  # safe literal %\n",
    "\n",
    "\n",
    "def flatten_dict(d: Mapping[str, Any], parent_key: str = \"\", sep: str = \".\") -> Dict[str, Any]:\n",
    "    \"\"\"Flatten nested dicts/lists/tuples into a flat mapping for logging.\"\"\"\n",
    "    flat: Dict[str, Any] = {}\n",
    "    for k, v in d.items():\n",
    "        nk = f\"{parent_key}{sep}{k}\" if parent_key else str(k)\n",
    "        if isinstance(v, Mapping):\n",
    "            flat.update(flatten_dict(v, parent_key=nk, sep=sep))\n",
    "        elif isinstance(v, (list, tuple)):\n",
    "            if (len(v) <= 8) and all(isinstance(x, (int, float, str, bool, type(None))) for x in v):\n",
    "                flat[nk] = list(v)\n",
    "            else:\n",
    "                flat[nk] = f\"{type(v).__name__}(len={len(v)})\"\n",
    "        else:\n",
    "            if isinstance(v, (np.integer, np.floating)):\n",
    "                v = v.item()\n",
    "            flat[nk] = v\n",
    "    return flat\n",
    "\n",
    "\n",
    "def compute_regression_metrics(y_true: pd.Series, y_pred: pd.Series) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Return dict of regression metrics (MSE, RMSE, MAE, R²).\n",
    "    Assumes alignment by index or reproducible alignment via concatenation.\n",
    "    \"\"\"\n",
    "    df = pd.concat([y_true.rename(\"y_true\"), y_pred.rename(\"y_pred\")], axis=1).dropna()\n",
    "    if df.empty:\n",
    "        return {\"mse\": np.nan, \"rmse\": np.nan, \"mae\": np.nan, \"r2\": np.nan}\n",
    "    err = df[\"y_pred\"] - df[\"y_true\"]\n",
    "    mse = float(np.mean(err ** 2))\n",
    "    rmse = float(math.sqrt(mse))\n",
    "    mae = float(np.mean(np.abs(err)))\n",
    "    var = float(np.var(df[\"y_true\"], ddof=0))\n",
    "    r2 = float(1 - mse / var) if var > 0 else np.nan\n",
    "    return {\"mse\": mse, \"rmse\": rmse, \"mae\": mae, \"r2\": r2}\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Core Logger\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class ModelRunLogger:\n",
    "    \"\"\"\n",
    "    TensorBoard-based, model-agnostic logger for regression workflows.\n",
    "\n",
    "    - Per-run directory management with timestamped naming\n",
    "    - Hyperparameter & config capture (JSON + scalars)\n",
    "    - Regression metrics logging\n",
    "    - Datetime-indexed time series logging (walltime axis)\n",
    "    - Multi-series logging\n",
    "    - Matplotlib figure logging\n",
    "    - JSON snapshots (config / metrics)\n",
    "    - Optional CSV export for each series\n",
    "\n",
    "    Typical usage:\n",
    "        with ModelRunLogger(\"my_model\") as logger:\n",
    "            logger.log_hparams(model_params=..., data_params=...)\n",
    "            logger.log_regression_metrics(y_train, y_train_pred, \"train\")\n",
    "            logger.log_regression_metrics(y_test, y_test_pred, \"test\")\n",
    "            logger.log_series_datetime(\"pnl\", pnl_series)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        base_run_dir: str | Path = \"runs\",\n",
    "        run_name: str | None = None,\n",
    "        group_by_date: bool = True,\n",
    "        overwrite: bool = False,\n",
    "        save_series_csv: bool = True,\n",
    "        extra_config: Dict[str, Any] | None = None,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        date_part = dt.date.today().isoformat() if group_by_date else \"\"\n",
    "        base_dir = Path(base_run_dir)\n",
    "        if date_part:\n",
    "            base_dir = base_dir / date_part\n",
    "        run_name = run_name or f\"{model_name}_{dt.datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        self.run_dir = base_dir / run_name\n",
    "\n",
    "        if overwrite and self.run_dir.exists():\n",
    "            shutil.rmtree(self.run_dir)\n",
    "        self.run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.writer = SummaryWriter(log_dir=str(self.run_dir))\n",
    "        self.config: Dict[str, Any] = {\"model_name\": model_name}\n",
    "        if extra_config:\n",
    "            self.config.update(extra_config)\n",
    "\n",
    "        self.metrics_history: Dict[str, Dict[str, float]] = {}\n",
    "        self.save_series_csv = save_series_csv\n",
    "        self._closed = False\n",
    "        self._start_time = time.time()\n",
    "\n",
    "        self._save_json(\"config.json\", self.config)\n",
    "\n",
    "    # ----- Config / Hyperparameters -----\n",
    "\n",
    "    def update_config(self, **kwargs):\n",
    "        self.config.update(kwargs)\n",
    "        self._save_json(\"config.json\", self.config)\n",
    "\n",
    "    def log_hparams(\n",
    "        self,\n",
    "        model_params: Dict[str, Any],\n",
    "        data_params: Dict[str, Any] | None = None,\n",
    "        tag_prefix: str = \"hparams\"\n",
    "    ):\n",
    "        combined = {\"model_params\": model_params}\n",
    "        if data_params:\n",
    "            combined[\"data_params\"] = data_params\n",
    "\n",
    "        flat = flatten_dict(combined)\n",
    "        for k, v in flat.items():\n",
    "            if isinstance(v, (int, float)) and not isinstance(v, bool):\n",
    "                self.writer.add_scalar(f\"{tag_prefix}/{k}\", v, global_step=0)\n",
    "            else:\n",
    "                self.writer.add_text(f\"{tag_prefix}_text/{k}\", str(v), global_step=0)\n",
    "\n",
    "        self.config.update(combined)\n",
    "        self._save_json(\"config.json\", self.config)\n",
    "\n",
    "    # ----- Metrics -----\n",
    "\n",
    "    def log_regression_metrics(\n",
    "        self,\n",
    "        y_true: pd.Series,\n",
    "        y_pred: pd.Series | np.ndarray | list,\n",
    "        split: str,\n",
    "        step: int | None = None,\n",
    "        extra_metrics: Dict[str, float] | None = None,\n",
    "        prefix: str | None = None,\n",
    "    ) -> Dict[str, float]:\n",
    "        if not isinstance(y_true, pd.Series):\n",
    "            raise TypeError(\"y_true must be a pandas Series.\")\n",
    "        if not isinstance(y_pred, pd.Series):\n",
    "            y_pred = pd.Series(y_pred, index=y_true.index)\n",
    "\n",
    "        metrics = compute_regression_metrics(y_true, y_pred)\n",
    "        if extra_metrics:\n",
    "            metrics.update(extra_metrics)\n",
    "\n",
    "        tag_root = f\"{prefix}/{split}\" if prefix else split\n",
    "        for k, v in metrics.items():\n",
    "            self.writer.add_scalar(f\"{tag_root}/{k}\", v, global_step=step)\n",
    "\n",
    "        hist_key = f\"{split}_{step if step is not None else 'final'}\"\n",
    "        self.metrics_history[hist_key] = metrics\n",
    "        self._save_json(\"metrics.json\", self.metrics_history)\n",
    "        return metrics\n",
    "\n",
    "    def log_manual_metric(self, name: str, value: float, split: str = \"custom\", step: int | None = None):\n",
    "        self.writer.add_scalar(f\"{split}/{name}\", float(value), global_step=step)\n",
    "\n",
    "    # ----- Time Series (walltime logging) -----\n",
    "\n",
    "    def log_series_datetime(\n",
    "        self,\n",
    "        name: str,\n",
    "        series: pd.Series,\n",
    "        group: str = \"timeseries\",\n",
    "        sort: bool = True,\n",
    "        step_offset: int = 0,\n",
    "        downsample: str | None = None,\n",
    "    ):\n",
    "        if not isinstance(series, pd.Series):\n",
    "            raise TypeError(\"series must be a pandas Series.\")\n",
    "        if not isinstance(series.index, pd.DatetimeIndex):\n",
    "            raise ValueError(\"Series must have a DatetimeIndex.\")\n",
    "\n",
    "        s = series.dropna()\n",
    "        if s.empty:\n",
    "            return\n",
    "        if s.index.tz is None:\n",
    "            s.index = s.index.tz_localize(\"UTC\")\n",
    "        else:\n",
    "            s.index = s.index.tz_convert(\"UTC\")\n",
    "        if sort:\n",
    "            s = s.sort_index()\n",
    "        if downsample:\n",
    "            s = s.resample(downsample).last().dropna()\n",
    "\n",
    "        tag = f\"{group}/{name}\"\n",
    "        for i, (ts, val) in enumerate(s.items()):\n",
    "            if pd.isna(val):\n",
    "                continue\n",
    "            step = step_offset + i\n",
    "            self.writer.add_scalar(tag, float(val), global_step=step, walltime=ts.timestamp())\n",
    "\n",
    "        if self.save_series_csv:\n",
    "            s.to_frame(name=name).to_csv(self.run_dir / f\"{name}.csv\", index_label=\"timestamp\")\n",
    "\n",
    "    def log_series_datetime_multi(\n",
    "        self,\n",
    "        name: str,\n",
    "        series_map: Dict[str, pd.Series],\n",
    "        group: str = \"timeseries_multi\",\n",
    "        align_inner: bool = True,\n",
    "        downsample: str | None = None,\n",
    "    ):\n",
    "        if align_inner:\n",
    "            df = pd.concat(series_map, axis=1).dropna(how=\"all\")\n",
    "            for label in df.columns.levels[0]:\n",
    "                s = df[label].dropna()\n",
    "                self.log_series_datetime(f\"{name}/{label}\", s, group=group, downsample=downsample)\n",
    "        else:\n",
    "            for label, s in series_map.items():\n",
    "                self.log_series_datetime(f\"{name}/{label}\", s, group=group, downsample=downsample)\n",
    "\n",
    "    # ----- Figures -----\n",
    "\n",
    "    def log_figure(self, fig, tag: str, step: int | None = None, close: bool = True):\n",
    "        self.writer.add_figure(tag, fig, global_step=step)\n",
    "        if close:\n",
    "            plt.close(fig)\n",
    "\n",
    "    def log_series_overlay_figure(\n",
    "        self,\n",
    "        series_map: Dict[str, pd.Series],\n",
    "        tag: str,\n",
    "        title: str = \"\",\n",
    "        ylabel: str = \"\",\n",
    "        normalize: bool = False,\n",
    "        step: int | None = None,\n",
    "        figsize=(10, 3),\n",
    "    ):\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        for label, s in series_map.items():\n",
    "            if not isinstance(s, pd.Series):\n",
    "                continue\n",
    "            if not isinstance(s.index, pd.DatetimeIndex):\n",
    "                raise ValueError(f\"Series '{label}' must have a DatetimeIndex.\")\n",
    "            t = s.dropna()\n",
    "            if t.empty:\n",
    "                continue\n",
    "            if t.index.tz is None:\n",
    "                t.index = t.index.tz_localize(\"UTC\")\n",
    "            else:\n",
    "                t.index = t.index.tz_convert(\"UTC\")\n",
    "            t = t.sort_index()\n",
    "            if normalize and t.iloc[0] != 0:\n",
    "                t = t / abs(t.iloc[0]) - 1\n",
    "            ax.plot(t.index, t.values, label=label)\n",
    "        ax.set_title(title or \"Series Overlay\")\n",
    "        if ylabel:\n",
    "            ax.set_ylabel(ylabel)\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.legend(loc=\"best\")\n",
    "        self.log_figure(fig, tag=tag, step=step, close=True)\n",
    "\n",
    "    # ----- Persistence helpers -----\n",
    "\n",
    "    def _save_json(self, filename: str, obj: Any):\n",
    "        with open(self.run_dir / filename, \"w\") as f:\n",
    "            json.dump(obj, f, indent=2, default=str)\n",
    "\n",
    "    # ----- Lifecycle -----\n",
    "\n",
    "    def close(self):\n",
    "        if self._closed:\n",
    "            return\n",
    "        runtime = time.time() - self._start_time\n",
    "        self.writer.add_scalar(\"meta/runtime_seconds\", runtime, global_step=0)\n",
    "        self.writer.flush()\n",
    "        self.writer.close()\n",
    "        self._closed = True\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, *_):\n",
    "        self.close()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Convenience wrapper: Option A (log_basic_run)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def log_basic_run(\n",
    "    *,\n",
    "    model,\n",
    "    model_name: str,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    model_params: dict | None = None,\n",
    "    pnl_series: pd.Series | None = None,\n",
    "    sharpe_series: pd.Series | None = None,\n",
    "    extra_series: dict[str, pd.Series] | None = None,\n",
    "    zone: str | None = None,\n",
    "    target: str | None = None,\n",
    "    base_run_dir: str | Path = \"runs\",\n",
    "    run_name: str | None = None,\n",
    "    performance_group: str = \"performance\",\n",
    "    make_overlay_figure: bool = True,\n",
    "    overlay_tag: str = \"figures/performance_overlay\",\n",
    "    overlay_ylabel: str = \"Value\",\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Convenience wrapper for common regression logging pattern.\n",
    "\n",
    "    Returns a summary dict with run_dir, metrics, and config.\n",
    "    \"\"\"\n",
    "    if model_params is None and hasattr(model, \"get_params\"):\n",
    "        try:\n",
    "            model_params = model.get_params()\n",
    "        except Exception:\n",
    "            model_params = {}\n",
    "\n",
    "    extra_config = {}\n",
    "    if zone is not None:\n",
    "        extra_config[\"zone\"] = zone\n",
    "    if target is not None:\n",
    "        extra_config[\"target\"] = target\n",
    "\n",
    "    with ModelRunLogger(\n",
    "        model_name,\n",
    "        extra_config=extra_config,\n",
    "        base_run_dir=base_run_dir,\n",
    "        run_name=run_name,\n",
    "    ) as logger:\n",
    "        logger.log_hparams(\n",
    "            model_params=model_params,\n",
    "            data_params={\"n_train\": len(X_train), \"n_test\": len(X_test)},\n",
    "        )\n",
    "\n",
    "        # Predictions\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        # Metrics\n",
    "        logger.log_regression_metrics(\n",
    "            y_train, pd.Series(y_train_pred, index=y_train.index), split=\"train\"\n",
    "        )\n",
    "        logger.log_regression_metrics(\n",
    "            y_test, pd.Series(y_test_pred, index=y_test.index), split=\"test\"\n",
    "        )\n",
    "\n",
    "        # Series logging\n",
    "        series_logged: Dict[str, pd.Series] = {}\n",
    "        if pnl_series is not None:\n",
    "            logger.log_series_datetime(\"pnl\", pnl_series, group=performance_group)\n",
    "            series_logged[\"pnl\"] = pnl_series\n",
    "        if sharpe_series is not None:\n",
    "            logger.log_series_datetime(\n",
    "                \"rolling_sharpe\", sharpe_series, group=performance_group\n",
    "            )\n",
    "            series_logged[\"rolling_sharpe\"] = sharpe_series\n",
    "        if extra_series:\n",
    "            for name, s in extra_series.items():\n",
    "                logger.log_series_datetime(name, s, group=performance_group)\n",
    "                series_logged[name] = s\n",
    "\n",
    "        if make_overlay_figure and series_logged:\n",
    "            logger.log_series_overlay_figure(\n",
    "                series_logged,\n",
    "                tag=overlay_tag,\n",
    "                title=\"Performance Overlay\",\n",
    "                ylabel=overlay_ylabel,\n",
    "            )\n",
    "\n",
    "        summary = {\n",
    "            \"run_dir\": str(logger.run_dir),\n",
    "            \"model_name\": model_name,\n",
    "            \"metrics\": logger.metrics_history,\n",
    "            \"config\": logger.config,\n",
    "        }\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fa1093",
   "metadata": {},
   "outputs": [],
   "source": [
    "mport pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "from tracking.model_run_logger import log_basic_run\n",
    "\n",
    "# Assume you already have time-indexed train/test sets\n",
    "# X_train, y_train, X_test, y_test (y_* are pd.Series with DatetimeIndex)\n",
    "\n",
    "lgbm_params = {\n",
    "    \"n_estimators\": 400,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 64,\n",
    "    \"subsample\": 0.9,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "model = LGBMRegressor(**lgbm_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Optional performance series you computed elsewhere\n",
    "pnl_series = pnl_series  # pd.Series, DatetimeIndex\n",
    "sharpe_series = sharpe_series  # pd.Series, DatetimeIndex\n",
    "\n",
    "summary = log_basic_run(\n",
    "    model=model,\n",
    "    model_name=\"zoneA_lgbm\",\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    model_params=lgbm_params,\n",
    "    pnl_series=pnl_series,\n",
    "    sharpe_series=sharpe_series,\n",
    "    zone=\"A\",\n",
    "    target=\"da_imb_spread\",\n",
    ")\n",
    "\n",
    "print(\"Run stored at:\", summary[\"run_dir\"])\n",
    "print(\"Test RMSE:\", summary[\"metrics\"][\"test_final\"][\"rmse\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
